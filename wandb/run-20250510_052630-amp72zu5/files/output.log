05/10/2025 05:26:32 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[WARNING|logging.py:314] 2025-05-10 05:26:41,786 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
flash_attention_2
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.22it/s]
Model dtype: torch.bfloat16
Number of layers (parameter sets) in the model: 291
max_gate_samples is 50
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
['query', 'response', 'type', 'original_question']
  0%|                                                                                                                                                                     | 0/1 [00:00<?, ?it/s][WARNING|logging.py:329] 2025-05-10 05:26:48,770 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
==========================================
True
==========================================
=================================================================================
Adamw
8
False
=================================================================================
=================================================================================
SchedulerType.COSINE
=================================================================================
create_optimizer
LoRARite init
checkpoint
None
LoRARite init
/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|                                                                                                                                                                     | 0/1 [00:09<?, ?it/s]
Traceback (most recent call last):
  File "/root/autodl-tmp/aslora_new/main_lora.py", line 852, in <module>
    main()
  File "/root/autodl-tmp/aslora_new/main_lora.py", line 753, in main
    train_result = trainer.train()#trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/trainer.py", line 1917, in _inner_training_loop
    self.optimizer.step()
  File "/root/miniconda3/lib/python3.10/site-packages/accelerate/optimizer.py", line 145, in step
    self.optimizer.step(closure)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/root/autodl-tmp/aslora_new/lora_rite.py", line 465, in step
    update_l = update_l + weight_decay * param_l
RuntimeError: The size of tensor a (8) must match the size of tensor b (4096) at non-singleton dimension 1
