05/10/2025 05:42:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[WARNING|logging.py:314] 2025-05-10 05:42:37,269 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
flash_attention_2
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.24it/s]
Model dtype: torch.bfloat16
Number of layers (parameter sets) in the model: 291
max_gate_samples is 50
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
['query', 'response', 'type', 'original_question']
  0%|                                                                                                                                                                     | 0/1 [00:00<?, ?it/s][WARNING|logging.py:329] 2025-05-10 05:42:44,667 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
==========================================
True
==========================================
=================================================================================
Adamw
8
False
=================================================================================
=================================================================================
SchedulerType.COSINE
=================================================================================
create_optimizer
LoRARite init
checkpoint
None
LoRARite init
/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
                                                                                                                                                                                                
                                                                                                                                                                                                
{'loss': 1.2652, 'learning_rate': 5e-06, 'epoch': 0.0}
{'loss': 1.1684, 'learning_rate': 1e-05, 'epoch': 0.01}
{'loss': 1.0464, 'learning_rate': 1.5e-05, 'epoch': 0.01}
{'loss': 0.9202, 'learning_rate': 2e-05, 'epoch': 0.01}
{'loss': 0.8006, 'learning_rate': 2.5e-05, 'epoch': 0.02}
{'loss': 0.7591, 'learning_rate': 3e-05, 'epoch': 0.02}
{'loss': 0.7386, 'learning_rate': 3.5e-05, 'epoch': 0.03}
{'loss': 0.7472, 'learning_rate': 4e-05, 'epoch': 0.03}
{'loss': 0.7223, 'learning_rate': 4.5e-05, 'epoch': 0.03}
{'loss': 0.7144, 'learning_rate': 5e-05, 'epoch': 0.04}
{'loss': 0.7043, 'learning_rate': 4.9996582624811725e-05, 'epoch': 0.04}
{'loss': 0.6854, 'learning_rate': 4.9986331433523156e-05, 'epoch': 0.04}
{'loss': 0.6917, 'learning_rate': 4.996924922870762e-05, 'epoch': 0.05}
{'loss': 0.6809, 'learning_rate': 4.994534068046937e-05, 'epoch': 0.05}
{'loss': 0.7014, 'learning_rate': 4.991461232516675e-05, 'epoch': 0.05}
{'loss': 0.6894, 'learning_rate': 4.9877072563625285e-05, 'epoch': 0.06}
{'loss': 0.6831, 'learning_rate': 4.9832731658840956e-05, 'epoch': 0.06}
{'loss': 0.6892, 'learning_rate': 4.978160173317438e-05, 'epoch': 0.07}
{'loss': 0.6838, 'learning_rate': 4.972369676503672e-05, 'epoch': 0.07}
{'loss': 0.6695, 'learning_rate': 4.965903258506806e-05, 'epoch': 0.07}
{'loss': 0.6757, 'learning_rate': 4.958762687180956e-05, 'epoch': 0.08}
{'loss': 0.6867, 'learning_rate': 4.9509499146870236e-05, 'epoch': 0.08}
{'loss': 0.6533, 'learning_rate': 4.9424670769589984e-05, 'epoch': 0.08}
{'loss': 0.6553, 'learning_rate': 4.933316493120015e-05, 'epoch': 0.09}
{'loss': 0.6663, 'learning_rate': 4.923500664848326e-05, 'epoch': 0.09}
{'loss': 0.6645, 'learning_rate': 4.913022275693372e-05, 'epoch': 0.09}
{'loss': 0.6641, 'learning_rate': 4.901884190342121e-05, 'epoch': 0.1}
{'loss': 0.6706, 'learning_rate': 4.8900894538358944e-05, 'epoch': 0.1}
{'loss': 0.6605, 'learning_rate': 4.877641290737884e-05, 'epoch': 0.11}
{'loss': 0.6571, 'learning_rate': 4.864543104251587e-05, 'epoch': 0.11}
{'loss': 0.6489, 'learning_rate': 4.850798475290403e-05, 'epoch': 0.11}
{'loss': 0.6661, 'learning_rate': 4.8364111614986527e-05, 'epoch': 0.12}
{'loss': 0.6706, 'learning_rate': 4.821385096224268e-05, 'epoch': 0.12}
{'loss': 0.6533, 'learning_rate': 4.805724387443462e-05, 'epoch': 0.12}
{'loss': 0.6556, 'learning_rate': 4.789433316637644e-05, 'epoch': 0.13}
{'loss': 0.6666, 'learning_rate': 4.7725163376229064e-05, 'epoch': 0.13}
{'loss': 0.6554, 'learning_rate': 4.754978075332398e-05, 'epoch': 0.13}
{'loss': 0.6676, 'learning_rate': 4.736823324551909e-05, 'epoch': 0.14}
{'loss': 0.6416, 'learning_rate': 4.71805704860903e-05, 'epoch': 0.14}
{'loss': 0.652, 'learning_rate': 4.698684378016222e-05, 'epoch': 0.15}
{'loss': 0.6627, 'learning_rate': 4.678710609068193e-05, 'epoch': 0.15}
{'loss': 0.6384, 'learning_rate': 4.6581412023939354e-05, 'epoch': 0.15}
{'loss': 0.6474, 'learning_rate': 4.6369817814638475e-05, 'epoch': 0.16}
{'loss': 0.6449, 'learning_rate': 4.6152381310523387e-05, 'epoch': 0.16}
{'loss': 0.6451, 'learning_rate': 4.592916195656322e-05, 'epoch': 0.16}
{'loss': 0.629, 'learning_rate': 4.5700220778700504e-05, 'epoch': 0.17}
{'loss': 0.6396, 'learning_rate': 4.546562036716732e-05, 'epoch': 0.17}
{'loss': 0.6291, 'learning_rate': 4.522542485937369e-05, 'epoch': 0.17}
{'loss': 0.6386, 'learning_rate': 4.497969992237312e-05, 'epoch': 0.18}
{'loss': 0.651, 'learning_rate': 4.4728512734909844e-05, 'epoch': 0.18}
{'loss': 0.6418, 'learning_rate': 4.4471931969052816e-05, 'epoch': 0.19}
{'loss': 0.6519, 'learning_rate': 4.421002777142148e-05, 'epoch': 0.19}
{'loss': 0.6296, 'learning_rate': 4.3942871744008374e-05, 'epoch': 0.19}
{'loss': 0.6348, 'learning_rate': 4.367053692460385e-05, 'epoch': 0.2}
{'loss': 0.6406, 'learning_rate': 4.3393097766828293e-05, 'epoch': 0.2}
{'loss': 0.6357, 'learning_rate': 4.311063011977723e-05, 'epoch': 0.2}
{'loss': 0.65, 'learning_rate': 4.282321120728493e-05, 'epoch': 0.21}
{'loss': 0.6354, 'learning_rate': 4.2530919606812216e-05, 'epoch': 0.21}
{'loss': 0.6203, 'learning_rate': 4.223383522796415e-05, 'epoch': 0.21}
{'loss': 0.6132, 'learning_rate': 4.193203929064353e-05, 'epoch': 0.22}
{'loss': 0.6226, 'learning_rate': 4.16256143028462e-05, 'epoch': 0.22}
{'loss': 0.6367, 'learning_rate': 4.131464403810422e-05, 'epoch': 0.23}
{'loss': 0.6239, 'learning_rate': 4.099921351258292e-05, 'epoch': 0.23}
{'loss': 0.6193, 'learning_rate': 4.067940896183843e-05, 'epoch': 0.23}
{'loss': 0.6305, 'learning_rate': 4.03553178172417e-05, 'epoch': 0.24}
{'loss': 0.6438, 'learning_rate': 4.002702868207563e-05, 'epoch': 0.24}
{'loss': 0.6409, 'learning_rate': 3.969463130731183e-05, 'epoch': 0.24}
{'loss': 0.6295, 'learning_rate': 3.935821656707359e-05, 'epoch': 0.25}
{'loss': 0.6277, 'learning_rate': 3.901787643379182e-05, 'epoch': 0.25}
{'loss': 0.6219, 'learning_rate': 3.867370395306068e-05, 'epoch': 0.25}
{'loss': 0.6505, 'learning_rate': 3.832579321819985e-05, 'epoch': 0.26}
{'loss': 0.6404, 'learning_rate': 3.797423934453038e-05, 'epoch': 0.26}
{'loss': 0.6255, 'learning_rate': 3.76191384433711e-05, 'epoch': 0.27}
{'loss': 0.6304, 'learning_rate': 3.726058759576271e-05, 'epoch': 0.27}
{'loss': 0.6297, 'learning_rate': 3.689868482592684e-05, 'epoch': 0.27}
{'loss': 0.6347, 'learning_rate': 3.65335290744672e-05, 'epoch': 0.28}
{'loss': 0.6044, 'learning_rate': 3.616522017132017e-05, 'epoch': 0.28}
{'loss': 0.644, 'learning_rate': 3.579385880846232e-05, 'epoch': 0.28}
{'loss': 0.6215, 'learning_rate': 3.5419546512382266e-05, 'epoch': 0.29}
{'loss': 0.6384, 'learning_rate': 3.504238561632424e-05, 'epoch': 0.29}
{'loss': 0.6135, 'learning_rate': 3.4662479232311306e-05, 'epoch': 0.29}
{'loss': 0.6327, 'learning_rate': 3.427993122295552e-05, 'epoch': 0.3}
{'loss': 0.6245, 'learning_rate': 3.389484617306292e-05, 'epoch': 0.3}
