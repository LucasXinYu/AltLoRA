05/05/2025 20:56:37 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[WARNING|logging.py:314] 2025-05-05 20:56:49,690 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
flash_attention_2
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.11it/s]
Model dtype: torch.bfloat16
Number of layers (parameter sets) in the model: 291
max_gate_samples is 50
trainable params: 29,360,128 || all params: 8,059,621,376 || trainable%: 0.3643
['query', 'response', 'type', 'original_question']
  0%|                                                                                                                                                                     | 0/1 [00:00<?, ?it/s][WARNING|logging.py:329] 2025-05-05 20:57:17,236 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
==========================================
True
==========================================
=================================================================================
Adamw
8
False
=================================================================================
=================================================================================
SchedulerType.COSINE
=================================================================================
checkpoint
None
/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.79s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.17s/it]
{'train_runtime': 7.7996, 'train_samples_per_second': 4.103, 'train_steps_per_second': 0.128, 'train_loss': 1.2542120218276978, 'epoch': 0.0}
***** train metrics *****
  epoch                    =        0.0
  train_loss               =     1.2542
  train_runtime            = 0:00:07.79
  train_samples            =      44056
  train_samples_per_second =      4.103
  train_steps_per_second   =      0.128

========== MODEL STRUCTURE ==========
PeftModelForCausalLM(
  (base_model): MoELoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): MoELinear(
                in_features=4096, out_features=4096, bias=False
                (lora_dropout): Dropout(p=0.05, inplace=False)
                (lora_route): Linear(in_features=4096, out_features=4, bias=False)
                (lora_As): ModuleList(
                  (0-3): 4 x Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_Bs): ModuleList(
                  (0-3): 4 x Linear(in_features=8, out_features=4096, bias=False)
                )
              )
              (k_proj): MoELinear(
                in_features=4096, out_features=1024, bias=False
                (lora_dropout): Dropout(p=0.05, inplace=False)
                (lora_route): Linear(in_features=4096, out_features=4, bias=False)
                (lora_As): ModuleList(
                  (0-3): 4 x Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_Bs): ModuleList(
                  (0-3): 4 x Linear(in_features=8, out_features=1024, bias=False)
                )
              )
              (v_proj): MoELinear(
                in_features=4096, out_features=1024, bias=False
                (lora_dropout): Dropout(p=0.05, inplace=False)
                (lora_route): Linear(in_features=4096, out_features=4, bias=False)
                (lora_As): ModuleList(
                  (0-3): 4 x Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_Bs): ModuleList(
                  (0-3): 4 x Linear(in_features=8, out_features=1024, bias=False)
                )
              )
              (o_proj): MoELinear(
                in_features=4096, out_features=4096, bias=False
                (lora_dropout): Dropout(p=0.05, inplace=False)
                (lora_route): Linear(in_features=4096, out_features=4, bias=False)
                (lora_As): ModuleList(
                  (0-3): 4 x Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_Bs): ModuleList(
                  (0-3): 4 x Linear(in_features=8, out_features=4096, bias=False)
                )
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)

========== TRAINABLE PARAMETERS ==========
base_model.model.model.layers.0.self_attn.q_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.0.self_attn.q_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_Bs.0.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_Bs.1.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_Bs.2.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_Bs.3.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.0.self_attn.k_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_Bs.0.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.k_proj.lora_Bs.1.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.k_proj.lora_Bs.2.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.k_proj.lora_Bs.3.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.v_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.0.self_attn.v_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.v_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.v_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.v_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.v_proj.lora_Bs.0.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.v_proj.lora_Bs.1.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.v_proj.lora_Bs.2.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.v_proj.lora_Bs.3.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.o_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.0.self_attn.o_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_Bs.0.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_Bs.1.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_Bs.2.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_Bs.3.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.1.self_attn.q_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_Bs.0.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_Bs.1.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_Bs.2.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_Bs.3.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.k_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.1.self_attn.k_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.k_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.k_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.k_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768

Total trainable parameters: 29,360,128 / 8,059,621,376 (0.36%)
[Replace] base_model.model.model.layers.0.self_attn.q_proj
[Replace] base_model.model.model.layers.0.self_attn.k_proj
[Replace] base_model.model.model.layers.0.self_attn.v_proj
[Replace] base_model.model.model.layers.0.self_attn.o_proj
[Replace] base_model.model.model.layers.1.self_attn.q_proj
[Replace] base_model.model.model.layers.1.self_attn.k_proj
[Replace] base_model.model.model.layers.1.self_attn.v_proj
[Replace] base_model.model.model.layers.1.self_attn.o_proj
[Replace] base_model.model.model.layers.2.self_attn.q_proj
[Replace] base_model.model.model.layers.2.self_attn.k_proj
[Replace] base_model.model.model.layers.2.self_attn.v_proj
[Replace] base_model.model.model.layers.2.self_attn.o_proj
[Replace] base_model.model.model.layers.3.self_attn.q_proj
[Replace] base_model.model.model.layers.3.self_attn.k_proj
[Replace] base_model.model.model.layers.3.self_attn.v_proj
[Replace] base_model.model.model.layers.3.self_attn.o_proj
[Replace] base_model.model.model.layers.4.self_attn.q_proj
[Replace] base_model.model.model.layers.4.self_attn.k_proj
[Replace] base_model.model.model.layers.4.self_attn.v_proj
[Replace] base_model.model.model.layers.4.self_attn.o_proj
[Replace] base_model.model.model.layers.5.self_attn.q_proj
[Replace] base_model.model.model.layers.5.self_attn.k_proj
[Replace] base_model.model.model.layers.5.self_attn.v_proj
[Replace] base_model.model.model.layers.5.self_attn.o_proj
[Replace] base_model.model.model.layers.6.self_attn.q_proj
[Replace] base_model.model.model.layers.6.self_attn.k_proj
[Replace] base_model.model.model.layers.6.self_attn.v_proj
[Replace] base_model.model.model.layers.6.self_attn.o_proj
[Replace] base_model.model.model.layers.7.self_attn.q_proj
[Replace] base_model.model.model.layers.7.self_attn.k_proj
[Replace] base_model.model.model.layers.7.self_attn.v_proj
[Replace] base_model.model.model.layers.7.self_attn.o_proj
[Replace] base_model.model.model.layers.8.self_attn.q_proj
[Replace] base_model.model.model.layers.8.self_attn.k_proj
[Replace] base_model.model.model.layers.8.self_attn.v_proj
[Replace] base_model.model.model.layers.8.self_attn.o_proj
[Replace] base_model.model.model.layers.9.self_attn.q_proj
[Replace] base_model.model.model.layers.9.self_attn.k_proj
[Replace] base_model.model.model.layers.9.self_attn.v_proj
[Replace] base_model.model.model.layers.9.self_attn.o_proj
[Replace] base_model.model.model.layers.10.self_attn.q_proj
[Replace] base_model.model.model.layers.10.self_attn.k_proj
[Replace] base_model.model.model.layers.10.self_attn.v_proj
[Replace] base_model.model.model.layers.10.self_attn.o_proj
[Replace] base_model.model.model.layers.11.self_attn.q_proj
[Replace] base_model.model.model.layers.11.self_attn.k_proj
[Replace] base_model.model.model.layers.11.self_attn.v_proj
[Replace] base_model.model.model.layers.11.self_attn.o_proj
[Replace] base_model.model.model.layers.12.self_attn.q_proj
[Replace] base_model.model.model.layers.12.self_attn.k_proj
[Replace] base_model.model.model.layers.12.self_attn.v_proj
[Replace] base_model.model.model.layers.12.self_attn.o_proj
[Replace] base_model.model.model.layers.13.self_attn.q_proj
[Replace] base_model.model.model.layers.13.self_attn.k_proj
[Replace] base_model.model.model.layers.13.self_attn.v_proj
[Replace] base_model.model.model.layers.13.self_attn.o_proj
[Replace] base_model.model.model.layers.14.self_attn.q_proj
[Replace] base_model.model.model.layers.14.self_attn.k_proj
[Replace] base_model.model.model.layers.14.self_attn.v_proj
[Replace] base_model.model.model.layers.14.self_attn.o_proj
[Replace] base_model.model.model.layers.15.self_attn.q_proj
[Replace] base_model.model.model.layers.15.self_attn.k_proj
[Replace] base_model.model.model.layers.15.self_attn.v_proj
[Replace] base_model.model.model.layers.15.self_attn.o_proj
[Replace] base_model.model.model.layers.16.self_attn.q_proj
[Replace] base_model.model.model.layers.16.self_attn.k_proj
[Replace] base_model.model.model.layers.16.self_attn.v_proj
[Replace] base_model.model.model.layers.16.self_attn.o_proj
[Replace] base_model.model.model.layers.17.self_attn.q_proj
[Replace] base_model.model.model.layers.17.self_attn.k_proj
[Replace] base_model.model.model.layers.17.self_attn.v_proj
[Replace] base_model.model.model.layers.17.self_attn.o_proj
[Replace] base_model.model.model.layers.18.self_attn.q_proj
[Replace] base_model.model.model.layers.18.self_attn.k_proj
[Replace] base_model.model.model.layers.18.self_attn.v_proj
[Replace] base_model.model.model.layers.18.self_attn.o_proj
[Replace] base_model.model.model.layers.19.self_attn.q_proj
[Replace] base_model.model.model.layers.19.self_attn.k_proj
[Replace] base_model.model.model.layers.19.self_attn.v_proj
[Replace] base_model.model.model.layers.19.self_attn.o_proj
[Replace] base_model.model.model.layers.20.self_attn.q_proj
[Replace] base_model.model.model.layers.20.self_attn.k_proj
[Replace] base_model.model.model.layers.20.self_attn.v_proj
[Replace] base_model.model.model.layers.20.self_attn.o_proj
[Replace] base_model.model.model.layers.21.self_attn.q_proj
[Replace] base_model.model.model.layers.21.self_attn.k_proj
[Replace] base_model.model.model.layers.21.self_attn.v_proj
[Replace] base_model.model.model.layers.21.self_attn.o_proj
[Replace] base_model.model.model.layers.22.self_attn.q_proj
[Replace] base_model.model.model.layers.22.self_attn.k_proj
[Replace] base_model.model.model.layers.22.self_attn.v_proj
[Replace] base_model.model.model.layers.22.self_attn.o_proj
[Replace] base_model.model.model.layers.23.self_attn.q_proj
[Replace] base_model.model.model.layers.23.self_attn.k_proj
[Replace] base_model.model.model.layers.23.self_attn.v_proj
[Replace] base_model.model.model.layers.23.self_attn.o_proj
[Replace] base_model.model.model.layers.24.self_attn.q_proj
[Replace] base_model.model.model.layers.24.self_attn.k_proj
[Replace] base_model.model.model.layers.24.self_attn.v_proj
[Replace] base_model.model.model.layers.24.self_attn.o_proj
[Replace] base_model.model.model.layers.25.self_attn.q_proj
[Replace] base_model.model.model.layers.25.self_attn.k_proj
[Replace] base_model.model.model.layers.25.self_attn.v_proj
[Replace] base_model.model.model.layers.25.self_attn.o_proj
[Replace] base_model.model.model.layers.26.self_attn.q_proj
[Replace] base_model.model.model.layers.26.self_attn.k_proj
[Replace] base_model.model.model.layers.26.self_attn.v_proj
[Replace] base_model.model.model.layers.26.self_attn.o_proj
[Replace] base_model.model.model.layers.27.self_attn.q_proj
[Replace] base_model.model.model.layers.27.self_attn.k_proj
[Replace] base_model.model.model.layers.27.self_attn.v_proj
[Replace] base_model.model.model.layers.27.self_attn.o_proj
[Replace] base_model.model.model.layers.28.self_attn.q_proj
[Replace] base_model.model.model.layers.28.self_attn.k_proj
[Replace] base_model.model.model.layers.28.self_attn.v_proj
[Replace] base_model.model.model.layers.28.self_attn.o_proj
[Replace] base_model.model.model.layers.29.self_attn.q_proj
[Replace] base_model.model.model.layers.29.self_attn.k_proj
[Replace] base_model.model.model.layers.29.self_attn.v_proj
[Replace] base_model.model.model.layers.29.self_attn.o_proj
[Replace] base_model.model.model.layers.30.self_attn.q_proj
[Replace] base_model.model.model.layers.30.self_attn.k_proj
[Replace] base_model.model.model.layers.30.self_attn.v_proj
[Replace] base_model.model.model.layers.30.self_attn.o_proj
[Replace] base_model.model.model.layers.31.self_attn.q_proj
[Replace] base_model.model.model.layers.31.self_attn.k_proj
[Replace] base_model.model.model.layers.31.self_attn.v_proj
[Replace] base_model.model.model.layers.31.self_attn.o_proj
[✔] Merged all MoE experts. Rebuilding clean LoraModel ...
Traceback (most recent call last):
  File "/root/autodl-tmp/aslora_new/main_lora_math_3_8b_moelora.py", line 984, in <module>
    main()
  File "/root/autodl-tmp/aslora_new/main_lora_math_3_8b_moelora.py", line 933, in main
    model = convert_moe_lora_to_lora(model)
  File "/root/autodl-tmp/aslora_new/main_lora_math_3_8b_moelora.py", line 911, in convert_moe_lora_to_lora
    clean_peft_model = get_peft_model(base_model, new_config)
  File "/root/autodl-tmp/aslora_new/peft/src/peft/mapping.py", line 184, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
  File "/root/autodl-tmp/aslora_new/peft/src/peft/peft_model.py", line 1520, in __init__
    super().__init__(model, peft_config, adapter_name, **kwargs)
  File "/root/autodl-tmp/aslora_new/peft/src/peft/peft_model.py", line 157, in __init__
    self.base_model = cls(model, {adapter_name: peft_config}, adapter_name)
  File "/root/autodl-tmp/aslora_new/peft/src/peft/tuners/lora/model.py", line 144, in __init__
    super().__init__(model, config, adapter_name)
  File "/root/autodl-tmp/aslora_new/peft/src/peft/tuners/tuners_utils.py", line 175, in __init__
    self.inject_adapter(self.model, adapter_name)
  File "/root/autodl-tmp/aslora_new/peft/src/peft/tuners/tuners_utils.py", line 431, in inject_adapter
    self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)
  File "/root/autodl-tmp/aslora_new/peft/src/peft/tuners/lora/model.py", line 229, in _create_and_replace
    new_module = self._create_new_module(lora_config, adapter_name, target, **kwargs)
  File "/root/autodl-tmp/aslora_new/peft/src/peft/tuners/lora/model.py", line 351, in _create_new_module
    raise ValueError(
ValueError: Target module LoraLinearWrapper(
  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
  (lora_A): Linear(in_features=4096, out_features=8, bias=False)
  (lora_B): Linear(in_features=8, out_features=4096, bias=False)
  (dropout): Dropout(p=0.0, inplace=False)
) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.
