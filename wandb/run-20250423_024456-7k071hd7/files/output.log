04/23/2025 02:44:58 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
2
/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Traceback (most recent call last):
  File "/root/autodl-tmp/aslora_new/main_lora_31_8b.py", line 843, in <module>
    main()
  File "/root/autodl-tmp/aslora_new/main_lora_31_8b.py", line 479, in main
    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **{k: v for k, v in config_kwargs.items() if k != "rope_scaling"})
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1124, in from_pretrained
    return config_class.from_dict(config_dict, **unused_kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/configuration_utils.py", line 764, in from_dict
    config = cls(**config_dict)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py", line 160, in __init__
    self._rope_scaling_validation()
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/models/llama/configuration_llama.py", line 180, in _rope_scaling_validation
    raise ValueError(
ValueError: `rope_scaling` must be a dictionary with with two fields, `type` and `factor`, got {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}
