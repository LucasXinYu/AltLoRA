05/06/2025 06:27:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[WARNING|logging.py:314] 2025-05-06 06:27:36,135 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
flash_attention_2
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.66it/s]
Model dtype: torch.bfloat16
Number of layers (parameter sets) in the model: 291
max_gate_samples is 50
trainable params: 29,360,128 || all params: 9,401,798,656 || trainable%: 0.3123
['query', 'response', 'type', 'original_question']
  0%|                                                                                                                                                                                                   | 0/1 [00:00<?, ?it/s][WARNING|logging.py:329] 2025-05-06 06:28:06,690 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
==========================================
True
==========================================
=================================================================================
Adamw
8
False
=================================================================================
=================================================================================
SchedulerType.COSINE
=================================================================================
checkpoint
None
/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [12:35<00:00,  7.56s/it]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [12:40<00:00, 760.50s/it]
{'loss': 1.2736, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 1.2633, 'learning_rate': 4.965903258506806e-05, 'epoch': 0.01}
{'loss': 1.2168, 'learning_rate': 4.864543104251587e-05, 'epoch': 0.01}
{'loss': 1.1647, 'learning_rate': 4.698684378016222e-05, 'epoch': 0.01}
{'loss': 1.0848, 'learning_rate': 4.4728512734909844e-05, 'epoch': 0.02}
{'loss': 1.0215, 'learning_rate': 4.193203929064353e-05, 'epoch': 0.02}
{'loss': 0.9708, 'learning_rate': 3.867370395306068e-05, 'epoch': 0.03}
{'loss': 0.93, 'learning_rate': 3.504238561632424e-05, 'epoch': 0.03}
{'loss': 0.8639, 'learning_rate': 3.1137137178519985e-05, 'epoch': 0.03}
{'loss': 0.8363, 'learning_rate': 2.7064483636808313e-05, 'epoch': 0.04}
{'loss': 0.8177, 'learning_rate': 2.2935516363191693e-05, 'epoch': 0.04}
{'loss': 0.7863, 'learning_rate': 1.8862862821480025e-05, 'epoch': 0.04}
{'loss': 0.7948, 'learning_rate': 1.495761438367577e-05, 'epoch': 0.05}
{'loss': 0.7822, 'learning_rate': 1.1326296046939333e-05, 'epoch': 0.05}
{'loss': 0.8043, 'learning_rate': 8.067960709356478e-06, 'epoch': 0.05}
{'loss': 0.7926, 'learning_rate': 5.271487265090163e-06, 'epoch': 0.06}
{'loss': 0.7875, 'learning_rate': 3.013156219837776e-06, 'epoch': 0.06}
{'loss': 0.7981, 'learning_rate': 1.3545689574841342e-06, 'epoch': 0.07}
{'loss': 0.7957, 'learning_rate': 3.4096741493194197e-07, 'epoch': 0.07}
{'loss': 0.7864, 'learning_rate': 0.0, 'epoch': 0.07}
{'train_runtime': 755.7969, 'train_samples_per_second': 4.234, 'train_steps_per_second': 0.132, 'train_loss': 0.9285630774497986, 'epoch': 0.07}
***** train metrics *****
  epoch                    =       0.07
  train_loss               =     0.9286
  train_runtime            = 0:12:35.79
  train_samples            =      44056
  train_samples_per_second =      4.234
  train_steps_per_second   =      0.132

========== MODEL STRUCTURE ==========
PeftModelForCausalLM(
  (base_model): MoELoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): MoELinear(
                in_features=4096, out_features=4096, bias=False
                (lora_dropout): Dropout(p=0.05, inplace=False)
                (lora_route): Linear(in_features=4096, out_features=4, bias=False)
                (lora_As): ModuleList(
                  (0-3): 4 x Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_Bs): ModuleList(
                  (0-3): 4 x Linear(in_features=8, out_features=4096, bias=False)
                )
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
              )
              (k_proj): MoELinear(
                in_features=4096, out_features=1024, bias=False
                (lora_dropout): Dropout(p=0.05, inplace=False)
                (lora_route): Linear(in_features=4096, out_features=4, bias=False)
                (lora_As): ModuleList(
                  (0-3): 4 x Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_Bs): ModuleList(
                  (0-3): 4 x Linear(in_features=8, out_features=1024, bias=False)
                )
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
              )
              (v_proj): MoELinear(
                in_features=4096, out_features=1024, bias=False
                (lora_dropout): Dropout(p=0.05, inplace=False)
                (lora_route): Linear(in_features=4096, out_features=4, bias=False)
                (lora_As): ModuleList(
                  (0-3): 4 x Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_Bs): ModuleList(
                  (0-3): 4 x Linear(in_features=8, out_features=1024, bias=False)
                )
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
              )
              (o_proj): MoELinear(
                in_features=4096, out_features=4096, bias=False
                (lora_dropout): Dropout(p=0.05, inplace=False)
                (lora_route): Linear(in_features=4096, out_features=4, bias=False)
                (lora_As): ModuleList(
                  (0-3): 4 x Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_Bs): ModuleList(
                  (0-3): 4 x Linear(in_features=8, out_features=4096, bias=False)
                )
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)

========== TRAINABLE PARAMETERS ==========
base_model.model.model.layers.0.self_attn.q_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.0.self_attn.q_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_Bs.0.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_Bs.1.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_Bs.2.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_Bs.3.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.0.self_attn.k_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_Bs.0.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.k_proj.lora_Bs.1.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.k_proj.lora_Bs.2.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.k_proj.lora_Bs.3.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.v_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.0.self_attn.v_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.v_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.v_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.v_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.v_proj.lora_Bs.0.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.v_proj.lora_Bs.1.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.v_proj.lora_Bs.2.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.v_proj.lora_Bs.3.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.o_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.0.self_attn.o_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_Bs.0.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_Bs.1.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_Bs.2.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_Bs.3.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.1.self_attn.q_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_Bs.0.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_Bs.1.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_Bs.2.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_Bs.3.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.k_proj.lora_route.weight: shape=torch.Size([4, 4096]), numel=16384
base_model.model.model.layers.1.self_attn.k_proj.lora_As.0.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.k_proj.lora_As.1.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.k_proj.lora_As.2.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.k_proj.lora_As.3.weight: shape=torch.Size([8, 4096]), numel=32768

Total trainable parameters: 29,360,128 / 9,401,798,656 (0.31%)

[MoELinear Layer] base_model.model.model.layers.0.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000088, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.000759
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000059, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.000820
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000028, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000006, std=0.000778
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000061, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.000828
  → Merged A: mean=0.000001, std=0.004517
  → Merged B: mean=-0.000004, std=0.000355

[MoELinear Layer] base_model.model.model.layers.0.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000035, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000015, std=0.000874
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000052, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000006, std=0.000935
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000080, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000026, std=0.001007
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000082, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000017, std=0.000885
  → Merged A: mean=-0.000045, std=0.004517
  → Merged B: mean=0.000009, std=0.000488

[MoELinear Layer] base_model.model.model.layers.0.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000043, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000007, std=0.000931
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000004, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000008, std=0.000973
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000025, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000016, std=0.000931
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000073, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000001, std=0.000931
  → Merged A: mean=0.000024, std=0.004517
  → Merged B: mean=0.000007, std=0.000437

[MoELinear Layer] base_model.model.model.layers.0.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000012, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.001015
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000035, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000007, std=0.000999
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000039, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.001015
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000019, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.000984
  → Merged A: mean=0.000026, std=0.004547
  → Merged B: mean=-0.000002, std=0.000483

[MoELinear Layer] base_model.model.model.layers.1.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000035, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000005, std=0.000916
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000016, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.000927
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000058, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000005, std=0.000866
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000019, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.000961
  → Merged A: mean=-0.000032, std=0.004517
  → Merged B: mean=0.000002, std=0.000477

[MoELinear Layer] base_model.model.model.layers.1.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000037, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000002, std=0.001060
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000011, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000000, std=0.000984
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000054, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.000954
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000004, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000007, std=0.001053
  → Merged A: mean=-0.000008, std=0.004517
  → Merged B: mean=-0.000001, std=0.000526

[MoELinear Layer] base_model.model.model.layers.1.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000066, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000005, std=0.001045
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000035, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000008, std=0.001076
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000080, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000001, std=0.001122
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000040, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000002, std=0.001030
  → Merged A: mean=-0.000004, std=0.004547
  → Merged B: mean=-0.000003, std=0.000591

[MoELinear Layer] base_model.model.model.layers.1.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000027, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001160
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000038, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.001190
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000050, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000007, std=0.001106
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000043, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001114
  → Merged A: mean=-0.000005, std=0.004517
  → Merged B: mean=-0.000004, std=0.000462

[MoELinear Layer] base_model.model.model.layers.2.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000028, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.000805
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000007, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.000854
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000047, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.000874
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000054, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.000843
  → Merged A: mean=-0.000004, std=0.004578
  → Merged B: mean=-0.000001, std=0.000500

[MoELinear Layer] base_model.model.model.layers.2.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000061, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000012, std=0.000851
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000055, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000011, std=0.000912
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000019, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.000893
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000043, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000003, std=0.000870
  → Merged A: mean=0.000014, std=0.004517
  → Merged B: mean=0.000000, std=0.000420

[MoELinear Layer] base_model.model.model.layers.2.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000038, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000021, std=0.000965
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000006, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000008, std=0.000942
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000004, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000014, std=0.000965
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000082, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000016, std=0.000992
  → Merged A: mean=0.000027, std=0.004517
  → Merged B: mean=-0.000007, std=0.000534

[MoELinear Layer] base_model.model.model.layers.2.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000026, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000007, std=0.000912
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000076, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.000904
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000031, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.000927
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000018, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.000912
  → Merged A: mean=0.000029, std=0.004547
  → Merged B: mean=-0.000002, std=0.000395

[MoELinear Layer] base_model.model.model.layers.3.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000047, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000011, std=0.001022
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000026, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000002, std=0.000893
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000028, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.000877
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000005, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.000900
  → Merged A: mean=-0.000013, std=0.004517
  → Merged B: mean=0.000003, std=0.000414

[MoELinear Layer] base_model.model.model.layers.3.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000043, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000002, std=0.000786
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000095, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000012, std=0.000965
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000010, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000001, std=0.000771
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000042, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000011, std=0.000916
  → Merged A: mean=-0.000047, std=0.004517
  → Merged B: mean=-0.000001, std=0.000418

[MoELinear Layer] base_model.model.model.layers.3.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000010, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000005, std=0.000904
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000032, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000014, std=0.000916
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000031, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.000977
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000003, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000010, std=0.000916
  → Merged A: mean=0.000004, std=0.004517
  → Merged B: mean=-0.000001, std=0.000362

[MoELinear Layer] base_model.model.model.layers.3.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000012, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000014, std=0.000942
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000009, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000007, std=0.000957
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000059, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000013, std=0.000969
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000016, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000013, std=0.000900
  → Merged A: mean=0.000014, std=0.004517
  → Merged B: mean=0.000008, std=0.000460

[MoELinear Layer] base_model.model.model.layers.4.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000118, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.000835
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000022, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.000832
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000072, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000009, std=0.001183
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000039, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000012, std=0.001045
  → Merged A: mean=0.000062, std=0.004547
  → Merged B: mean=-0.000003, std=0.000534

[MoELinear Layer] base_model.model.model.layers.4.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000048, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.000923
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000041, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000000, std=0.000992
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000002, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000009, std=0.001030
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000031, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000001, std=0.000889
  → Merged A: mean=0.000015, std=0.004547
  → Merged B: mean=-0.000004, std=0.000553

[MoELinear Layer] base_model.model.model.layers.4.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000108, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000026, std=0.000946
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000003, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000018, std=0.001038
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000076, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000002, std=0.000984
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000051, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000027, std=0.000992
  → Merged A: mean=0.000033, std=0.004547
  → Merged B: mean=0.000004, std=0.000542

[MoELinear Layer] base_model.model.model.layers.4.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000018, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.001022
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000031, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000005, std=0.001038
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000023, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001076
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000078, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000012, std=0.000992
  → Merged A: mean=-0.000022, std=0.004547
  → Merged B: mean=-0.000003, std=0.000584

[MoELinear Layer] base_model.model.model.layers.5.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000017, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.000946
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000122, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000008, std=0.001114
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000049, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001022
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000033, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.000908
  → Merged A: mean=0.000055, std=0.004517
  → Merged B: mean=0.000001, std=0.000441

[MoELinear Layer] base_model.model.model.layers.5.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000081, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000008, std=0.000935
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000082, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000011, std=0.001076
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000043, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000005, std=0.001068
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000050, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000016, std=0.000938
  → Merged A: mean=-0.000023, std=0.004547
  → Merged B: mean=-0.000004, std=0.000479

[MoELinear Layer] base_model.model.model.layers.5.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000022, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.000992
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000019, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000032, std=0.001045
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000034, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000002, std=0.001007
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000075, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000006, std=0.001129
  → Merged A: mean=-0.000000, std=0.004547
  → Merged B: mean=-0.000007, std=0.000471

[MoELinear Layer] base_model.model.model.layers.5.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000039, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001038
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000002, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000006, std=0.001030
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000008, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001045
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000013, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001038
  → Merged A: mean=-0.000015, std=0.004578
  → Merged B: mean=-0.000001, std=0.000690

[MoELinear Layer] base_model.model.model.layers.6.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000041, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.000832
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000028, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001007
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000008, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001122
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000037, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.000973
  → Merged A: mean=-0.000025, std=0.004547
  → Merged B: mean=-0.000000, std=0.000507

[MoELinear Layer] base_model.model.model.layers.6.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000026, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000002, std=0.001007
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000077, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000002, std=0.001083
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000027, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000012, std=0.000984
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000041, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.001030
  → Merged A: mean=0.000030, std=0.004547
  → Merged B: mean=-0.000003, std=0.000530

[MoELinear Layer] base_model.model.model.layers.6.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000027, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000005, std=0.000999
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000041, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000007, std=0.001053
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000018, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000009, std=0.001015
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000032, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000015, std=0.000965
  → Merged A: mean=0.000021, std=0.004578
  → Merged B: mean=0.000003, std=0.000507

[MoELinear Layer] base_model.model.model.layers.6.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000020, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001053
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000061, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.001045
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000033, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000006, std=0.001045
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000051, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000007, std=0.001022
  → Merged A: mean=0.000025, std=0.004547
  → Merged B: mean=0.000002, std=0.000530

[MoELinear Layer] base_model.model.model.layers.7.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000041, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.000942
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000039, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000010, std=0.000854
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000009, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001030
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000038, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001045
  → Merged A: mean=-0.000008, std=0.004517
  → Merged B: mean=0.000003, std=0.000469

[MoELinear Layer] base_model.model.model.layers.7.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000036, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.000820
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000049, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000003, std=0.001038
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000007, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.000858
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000015, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000008, std=0.001137
  → Merged A: mean=-0.000002, std=0.004547
  → Merged B: mean=0.000003, std=0.000378

[MoELinear Layer] base_model.model.model.layers.7.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000007, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000009, std=0.000954
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000026, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000009, std=0.001022
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000055, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.000908
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000036, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000039, std=0.001091
  → Merged A: mean=0.000028, std=0.004517
  → Merged B: mean=0.000009, std=0.000387

[MoELinear Layer] base_model.model.model.layers.7.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000021, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001022
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000028, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001015
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000101, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001015
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000046, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000012, std=0.001030
  → Merged A: mean=-0.000015, std=0.004547
  → Merged B: mean=0.000002, std=0.000467

[MoELinear Layer] base_model.model.model.layers.8.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000040, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000954
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000004, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001015
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000019, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.000942
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000024, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.000828
  → Merged A: mean=0.000019, std=0.004547
  → Merged B: mean=-0.000002, std=0.000504

[MoELinear Layer] base_model.model.model.layers.8.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000008, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.000767
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000074, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000015, std=0.000877
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000028, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000843
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000090, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.001091
  → Merged A: mean=-0.000046, std=0.004517
  → Merged B: mean=0.000004, std=0.000469

[MoELinear Layer] base_model.model.model.layers.8.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000016, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.000954
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000093, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000005, std=0.000992
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000031, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000005, std=0.000957
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000021, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.001099
  → Merged A: mean=-0.000022, std=0.004547
  → Merged B: mean=-0.000005, std=0.000465

[MoELinear Layer] base_model.model.model.layers.8.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000002, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.000973
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000093, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001007
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000033, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001007
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000068, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.000977
  → Merged A: mean=0.000015, std=0.004517
  → Merged B: mean=0.000001, std=0.000416

[MoELinear Layer] base_model.model.model.layers.9.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000040, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000000, std=0.001007
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000064, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.000828
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000046, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.000862
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000056, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.000946
  → Merged A: mean=-0.000028, std=0.004547
  → Merged B: mean=0.000000, std=0.000360

[MoELinear Layer] base_model.model.model.layers.9.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000027, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000003, std=0.000908
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000005, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000033, std=0.000938
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000100, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000000, std=0.000793
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000065, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.000946
  → Merged A: mean=-0.000036, std=0.004517
  → Merged B: mean=-0.000007, std=0.000439

[MoELinear Layer] base_model.model.model.layers.9.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000101, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000001, std=0.000969
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000038, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000018, std=0.000935
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000006, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000016, std=0.001060
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000062, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000015, std=0.000916
  → Merged A: mean=-0.000001, std=0.004517
  → Merged B: mean=0.000004, std=0.000374

[MoELinear Layer] base_model.model.model.layers.9.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000033, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.000999
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000006, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001053
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000020, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001015
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000011, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000002, std=0.001015
  → Merged A: mean=0.000001, std=0.004547
  → Merged B: mean=0.000000, std=0.000553

[MoELinear Layer] base_model.model.model.layers.10.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000020, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.000816
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000061, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.000885
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000139, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000007, std=0.000820
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000013, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.000843
  → Merged A: mean=-0.000022, std=0.004547
  → Merged B: mean=-0.000003, std=0.000412

[MoELinear Layer] base_model.model.model.layers.10.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000076, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000012, std=0.000793
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000044, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000007, std=0.000790
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000023, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.000824
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000052, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000007, std=0.000904
  → Merged A: mean=-0.000027, std=0.004486
  → Merged B: mean=-0.000000, std=0.000341

[MoELinear Layer] base_model.model.model.layers.10.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000030, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.000957
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000112, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000011, std=0.001007
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000044, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000012, std=0.001060
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000048, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.000961
  → Merged A: mean=-0.000013, std=0.004547
  → Merged B: mean=-0.000008, std=0.000504

[MoELinear Layer] base_model.model.model.layers.10.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000026, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000002, std=0.000999
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000004, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001060
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000004, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001053
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000037, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001015
  → Merged A: mean=-0.000018, std=0.004578
  → Merged B: mean=0.000001, std=0.000557

[MoELinear Layer] base_model.model.model.layers.11.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000011, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000000, std=0.000999
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000051, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000004, std=0.000851
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000025, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.000805
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000005, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000009, std=0.000973
  → Merged A: mean=-0.000003, std=0.004578
  → Merged B: mean=0.000004, std=0.000456

[MoELinear Layer] base_model.model.model.layers.11.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000046, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000006, std=0.000847
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000036, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.000862
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000005, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000009, std=0.000961
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000011, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000016, std=0.000931
  → Merged A: mean=-0.000001, std=0.004547
  → Merged B: mean=-0.000002, std=0.000534

[MoELinear Layer] base_model.model.model.layers.11.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000035, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000024, std=0.001083
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000002, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000013, std=0.000931
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000052, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000021, std=0.000965
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000073, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000013, std=0.001091
  → Merged A: mean=-0.000022, std=0.004578
  → Merged B: mean=0.000011, std=0.000557

[MoELinear Layer] base_model.model.model.layers.11.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000098, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000000, std=0.001114
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000041, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000008, std=0.001106
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000147, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000007, std=0.001106
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000096, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001106
  → Merged A: mean=0.000026, std=0.004517
  → Merged B: mean=0.000001, std=0.000484

[MoELinear Layer] base_model.model.model.layers.12.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000007, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000012, std=0.001091
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000099, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.000950
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000076, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000014, std=0.000923
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000044, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000007, std=0.000973
  → Merged A: mean=0.000003, std=0.004608
  → Merged B: mean=0.000008, std=0.000565

[MoELinear Layer] base_model.model.model.layers.12.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000018, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.000923
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000036, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000007, std=0.001053
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000103, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000008, std=0.001030
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000036, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000007, std=0.000874
  → Merged A: mean=0.000013, std=0.004517
  → Merged B: mean=-0.000003, std=0.000406

[MoELinear Layer] base_model.model.model.layers.12.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000022, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000008, std=0.000992
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000105, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000008, std=0.001152
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000051, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000010, std=0.001091
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000110, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000006, std=0.000984
  → Merged A: mean=0.000046, std=0.004578
  → Merged B: mean=-0.000001, std=0.000565

[MoELinear Layer] base_model.model.model.layers.12.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000139, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000008, std=0.001122
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000002, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001091
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000055, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001099
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000035, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001053
  → Merged A: mean=-0.000039, std=0.004547
  → Merged B: mean=-0.000002, std=0.000530

[MoELinear Layer] base_model.model.model.layers.13.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000026, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000006, std=0.000919
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000000, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.000984
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000043, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.000973
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000060, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000011, std=0.000931
  → Merged A: mean=0.000003, std=0.004517
  → Merged B: mean=-0.000002, std=0.000404

[MoELinear Layer] base_model.model.model.layers.13.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000095, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000037, std=0.000923
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000048, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.000862
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000010, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000005, std=0.000999
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000020, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000009, std=0.000931
  → Merged A: mean=0.000019, std=0.004547
  → Merged B: mean=0.000007, std=0.000496

[MoELinear Layer] base_model.model.model.layers.13.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000053, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.000984
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000017, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000012, std=0.000999
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000046, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000019, std=0.001137
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000107, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000015, std=0.000973
  → Merged A: mean=-0.000033, std=0.004578
  → Merged B: mean=-0.000012, std=0.000568

[MoELinear Layer] base_model.model.model.layers.13.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000078, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001106
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000091, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001099
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000019, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000008, std=0.001091
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000039, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000002, std=0.001030
  → Merged A: mean=0.000008, std=0.004578
  → Merged B: mean=0.000001, std=0.000538

[MoELinear Layer] base_model.model.model.layers.14.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000068, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.000957
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000048, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001068
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000035, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000005, std=0.001045
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000008, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000000, std=0.000916
  → Merged A: mean=-0.000040, std=0.004578
  → Merged B: mean=0.000001, std=0.000610

[MoELinear Layer] base_model.model.model.layers.14.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000095, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000001, std=0.001167
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000016, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000001, std=0.000977
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000057, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.000916
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000046, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000007, std=0.001091
  → Merged A: mean=0.000025, std=0.004578
  → Merged B: mean=0.000000, std=0.000511

[MoELinear Layer] base_model.model.model.layers.14.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000052, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000010, std=0.001076
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000072, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000014, std=0.001091
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000042, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000006, std=0.000999
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000035, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.000969
  → Merged A: mean=0.000024, std=0.004517
  → Merged B: mean=0.000002, std=0.000450

[MoELinear Layer] base_model.model.model.layers.14.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000053, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.001068
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000060, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.001076
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000072, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000005, std=0.001091
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000046, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001106
  → Merged A: mean=0.000035, std=0.004517
  → Merged B: mean=0.000000, std=0.000412

[MoELinear Layer] base_model.model.model.layers.15.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000054, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001213
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000064, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.000900
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000015, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001007
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000020, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.000957
  → Merged A: mean=0.000004, std=0.004578
  → Merged B: mean=-0.000002, std=0.000546

[MoELinear Layer] base_model.model.model.layers.15.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000022, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000009, std=0.001007
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000089, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000002, std=0.000893
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000165, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000015, std=0.001114
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000064, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.001129
  → Merged A: mean=-0.000074, std=0.004578
  → Merged B: mean=-0.000002, std=0.000496

[MoELinear Layer] base_model.model.model.layers.15.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000006, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.001030
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000050, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.001060
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000024, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000007, std=0.001129
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000013, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000032, std=0.000999
  → Merged A: mean=-0.000002, std=0.004578
  → Merged B: mean=0.000003, std=0.000603

[MoELinear Layer] base_model.model.model.layers.15.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000006, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.001152
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000027, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001122
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000030, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001137
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000085, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000005, std=0.001122
  → Merged A: mean=-0.000023, std=0.004547
  → Merged B: mean=0.000001, std=0.000542

[MoELinear Layer] base_model.model.model.layers.16.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000043, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.001198
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000094, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000006, std=0.001053
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000033, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.000969
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000068, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001060
  → Merged A: mean=0.000038, std=0.004547
  → Merged B: mean=-0.000003, std=0.000530

[MoELinear Layer] base_model.model.model.layers.16.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000023, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000003, std=0.001244
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000062, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.001114
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000037, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000001, std=0.001007
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000059, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000015, std=0.000961
  → Merged A: mean=0.000034, std=0.004517
  → Merged B: mean=-0.000003, std=0.000418

[MoELinear Layer] base_model.model.model.layers.16.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000003, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000010, std=0.001251
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000015, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000002, std=0.001053
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000016, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000001, std=0.001144
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000004, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000015, std=0.001060
  → Merged A: mean=-0.000006, std=0.004578
  → Merged B: mean=-0.000002, std=0.000486

[MoELinear Layer] base_model.model.model.layers.16.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000041, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001213
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000041, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.001198
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000018, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.001221
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000040, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000009, std=0.001228
  → Merged A: mean=-0.000005, std=0.004547
  → Merged B: mean=-0.000003, std=0.000572

[MoELinear Layer] base_model.model.model.layers.17.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000030, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000006, std=0.001114
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000043, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000002, std=0.001244
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000012, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000005, std=0.001038
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000061, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001228
  → Merged A: mean=0.000030, std=0.004578
  → Merged B: mean=0.000002, std=0.000538

[MoELinear Layer] base_model.model.model.layers.17.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000111, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000003, std=0.001137
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000005, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000001, std=0.001091
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000057, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000009, std=0.001198
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000049, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000023, std=0.001312
  → Merged A: mean=-0.000027, std=0.004578
  → Merged B: mean=0.000009, std=0.000595

[MoELinear Layer] base_model.model.model.layers.17.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000086, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000010, std=0.001282
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000086, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000010, std=0.001160
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000092, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000034, std=0.001160
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000005, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000037, std=0.001259
  → Merged A: mean=0.000018, std=0.004578
  → Merged B: mean=-0.000004, std=0.000584

[MoELinear Layer] base_model.model.model.layers.17.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000036, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000004, std=0.001228
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000076, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001251
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000056, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001205
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000010, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000000, std=0.001244
  → Merged A: mean=-0.000040, std=0.004547
  → Merged B: mean=0.000000, std=0.000576

[MoELinear Layer] base_model.model.model.layers.18.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000010, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000004, std=0.001244
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000015, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000004, std=0.001030
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000026, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001190
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000026, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.001030
  → Merged A: mean=-0.000006, std=0.004608
  → Merged B: mean=0.000001, std=0.000656

[MoELinear Layer] base_model.model.model.layers.18.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000005, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000016, std=0.001083
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000049, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000007, std=0.001053
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000044, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000010, std=0.001320
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000059, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000007, std=0.001366
  → Merged A: mean=0.000015, std=0.004547
  → Merged B: mean=-0.000002, std=0.000576

[MoELinear Layer] base_model.model.model.layers.18.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000014, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.001328
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000071, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000001, std=0.001305
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000035, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.001198
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000047, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000012, std=0.001129
  → Merged A: mean=-0.000001, std=0.004608
  → Merged B: mean=-0.000003, std=0.000656

[MoELinear Layer] base_model.model.model.layers.18.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000051, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000004, std=0.001259
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000074, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000007, std=0.001259
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000024, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001312
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000019, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000011, std=0.001190
  → Merged A: mean=0.000021, std=0.004517
  → Merged B: mean=-0.000005, std=0.000481

[MoELinear Layer] base_model.model.model.layers.19.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000038, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000008, std=0.001167
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000008, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001190
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000101, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001183
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000001, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000009, std=0.001114
  → Merged A: mean=0.000036, std=0.004547
  → Merged B: mean=-0.000001, std=0.000519

[MoELinear Layer] base_model.model.model.layers.19.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000052, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000018, std=0.001144
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000056, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000008, std=0.001167
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000043, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000019, std=0.000977
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000097, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000026, std=0.001205
  → Merged A: mean=-0.000013, std=0.004547
  → Merged B: mean=-0.000004, std=0.000576

[MoELinear Layer] base_model.model.model.layers.19.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000104, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.001343
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000013, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000009, std=0.001228
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000043, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000023, std=0.001328
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000055, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000001, std=0.001190
  → Merged A: mean=0.000019, std=0.004608
  → Merged B: mean=0.000004, std=0.000717

[MoELinear Layer] base_model.model.model.layers.19.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000018, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000011, std=0.001236
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000090, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.001244
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000022, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000010, std=0.001259
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000048, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000010, std=0.001251
  → Merged A: mean=0.000036, std=0.004608
  → Merged B: mean=0.000002, std=0.000587

[MoELinear Layer] base_model.model.model.layers.20.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000010, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.001022
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000076, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.001236
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000028, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001205
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000025, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000007, std=0.001152
  → Merged A: mean=0.000030, std=0.004547
  → Merged B: mean=-0.000002, std=0.000511

[MoELinear Layer] base_model.model.model.layers.20.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000014, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000012, std=0.001122
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000003, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000002, std=0.001236
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000060, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000021, std=0.001175
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000055, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000024, std=0.001221
  → Merged A: mean=-0.000004, std=0.004547
  → Merged B: mean=-0.000004, std=0.000584

[MoELinear Layer] base_model.model.model.layers.20.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000031, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.001205
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000069, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000005, std=0.001373
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000063, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000010, std=0.001221
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000071, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000005, std=0.001328
  → Merged A: mean=0.000024, std=0.004547
  → Merged B: mean=0.000001, std=0.000557

[MoELinear Layer] base_model.model.model.layers.20.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000003, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.001251
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000115, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000004, std=0.001259
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000005, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000015, std=0.001259
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000014, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000006, std=0.001221
  → Merged A: mean=-0.000030, std=0.004547
  → Merged B: mean=-0.000006, std=0.000526

[MoELinear Layer] base_model.model.model.layers.21.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000006, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.000984
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000049, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000008, std=0.001083
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000040, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001137
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000049, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001205
  → Merged A: mean=0.000013, std=0.004547
  → Merged B: mean=0.000003, std=0.000523

[MoELinear Layer] base_model.model.model.layers.21.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000094, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000017, std=0.001282
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000032, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000016, std=0.001076
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000000, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.000999
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000027, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.000923
  → Merged A: mean=-0.000038, std=0.004517
  → Merged B: mean=-0.000008, std=0.000557

[MoELinear Layer] base_model.model.model.layers.21.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000022, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000013, std=0.001320
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000062, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000007, std=0.001190
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000015, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000026, std=0.001312
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000015, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.001183
  → Merged A: mean=-0.000002, std=0.004547
  → Merged B: mean=-0.000009, std=0.000633

[MoELinear Layer] base_model.model.model.layers.21.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000056, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001289
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000107, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000010, std=0.001282
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000026, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000005, std=0.001244
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000044, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000011, std=0.001289
  → Merged A: mean=-0.000017, std=0.004608
  → Merged B: mean=-0.000003, std=0.000702

[MoELinear Layer] base_model.model.model.layers.22.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000076, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001213
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000010, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001251
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000011, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001045
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000053, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000002, std=0.001282
  → Merged A: mean=-0.000032, std=0.004547
  → Merged B: mean=0.000001, std=0.000546

[MoELinear Layer] base_model.model.model.layers.22.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000013, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.001289
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000053, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.001114
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000052, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000010, std=0.001190
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000101, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000009, std=0.001427
  → Merged A: mean=0.000029, std=0.004547
  → Merged B: mean=-0.000007, std=0.000656

[MoELinear Layer] base_model.model.model.layers.22.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000011, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000013, std=0.001312
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000024, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.001404
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000029, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000008, std=0.001259
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000087, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000034, std=0.001358
  → Merged A: mean=-0.000038, std=0.004547
  → Merged B: mean=0.000011, std=0.000610

[MoELinear Layer] base_model.model.model.layers.22.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000025, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.001343
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000012, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001358
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000011, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000008, std=0.001328
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000082, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.001335
  → Merged A: mean=-0.000026, std=0.004517
  → Merged B: mean=-0.000003, std=0.000473

[MoELinear Layer] base_model.model.model.layers.23.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000065, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.001038
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000036, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001205
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000060, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001228
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000027, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000007, std=0.000942
  → Merged A: mean=0.000034, std=0.004547
  → Merged B: mean=-0.000001, std=0.000549

[MoELinear Layer] base_model.model.model.layers.23.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000004, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000005, std=0.001099
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000062, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000011, std=0.001244
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000076, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.001122
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000026, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000010, std=0.001175
  → Merged A: mean=0.000004, std=0.004547
  → Merged B: mean=-0.000005, std=0.000546

[MoELinear Layer] base_model.model.model.layers.23.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000001, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.001381
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000017, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.001221
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000014, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000025, std=0.001350
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000052, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000003, std=0.001289
  → Merged A: mean=-0.000021, std=0.004547
  → Merged B: mean=0.000004, std=0.000546

[MoELinear Layer] base_model.model.model.layers.23.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000082, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001358
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000046, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000009, std=0.001282
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000019, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001297
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000034, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000006, std=0.001289
  → Merged A: mean=-0.000005, std=0.004639
  → Merged B: mean=-0.000001, std=0.000782

[MoELinear Layer] base_model.model.model.layers.24.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000010, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001022
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000207, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001175
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000014, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000000, std=0.001167
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000081, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001289
  → Merged A: mean=-0.000078, std=0.004578
  → Merged B: mean=-0.000002, std=0.000618

[MoELinear Layer] base_model.model.model.layers.24.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000062, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000008, std=0.001328
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000059, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.001175
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000052, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000001, std=0.001137
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000027, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000005, std=0.001205
  → Merged A: mean=0.000019, std=0.004578
  → Merged B: mean=-0.000001, std=0.000610

[MoELinear Layer] base_model.model.model.layers.24.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000056, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000046, std=0.001244
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000015, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000015, std=0.001312
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000047, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000029, std=0.001350
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000075, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000013, std=0.001450
  → Merged A: mean=0.000025, std=0.004578
  → Merged B: mean=-0.000004, std=0.000706

[MoELinear Layer] base_model.model.model.layers.24.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000046, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000002, std=0.001366
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000057, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001350
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000121, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000007, std=0.001442
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000026, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001381
  → Merged A: mean=-0.000049, std=0.004608
  → Merged B: mean=0.000000, std=0.000771

[MoELinear Layer] base_model.model.model.layers.25.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000049, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000006, std=0.001038
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000023, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.001366
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000052, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000004, std=0.000942
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000088, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001167
  → Merged A: mean=0.000042, std=0.004517
  → Merged B: mean=-0.000001, std=0.000484

[MoELinear Layer] base_model.model.model.layers.25.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000082, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000016, std=0.001518
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000010, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000002, std=0.001366
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000054, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000000, std=0.001244
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000079, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.001060
  → Merged A: mean=-0.000011, std=0.004608
  → Merged B: mean=-0.000004, std=0.000679

[MoELinear Layer] base_model.model.model.layers.25.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000050, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000026, std=0.001289
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000001, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.001205
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000072, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000004, std=0.001358
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000030, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000001, std=0.001228
  → Merged A: mean=0.000038, std=0.004608
  → Merged B: mean=-0.000006, std=0.000706

[MoELinear Layer] base_model.model.model.layers.25.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000044, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001373
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000047, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001419
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000043, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001335
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000009, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000005, std=0.001366
  → Merged A: mean=0.000008, std=0.004578
  → Merged B: mean=0.000001, std=0.000706

[MoELinear Layer] base_model.model.model.layers.26.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000056, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001274
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000074, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000000, std=0.001251
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000150, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001175
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000082, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000006, std=0.001083
  → Merged A: mean=-0.000013, std=0.004547
  → Merged B: mean=-0.000002, std=0.000515

[MoELinear Layer] base_model.model.model.layers.26.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000022, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000007, std=0.001099
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000103, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.001183
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000016, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000003, std=0.001297
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000019, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000008, std=0.001419
  → Merged A: mean=0.000022, std=0.004547
  → Merged B: mean=0.000003, std=0.000637

[MoELinear Layer] base_model.model.model.layers.26.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000008, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000007, std=0.001259
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000053, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.001442
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000013, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000007, std=0.001350
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000048, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000007, std=0.001442
  → Merged A: mean=0.000020, std=0.004669
  → Merged B: mean=-0.000003, std=0.000870

[MoELinear Layer] base_model.model.model.layers.26.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000086, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000002, std=0.001427
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000066, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000003, std=0.001350
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000028, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001373
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000019, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001373
  → Merged A: mean=-0.000050, std=0.004517
  → Merged B: mean=0.000000, std=0.000473

[MoELinear Layer] base_model.model.model.layers.27.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000080, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000005, std=0.001015
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000050, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001320
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000038, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000010, std=0.001053
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000021, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000008, std=0.001297
  → Merged A: mean=-0.000012, std=0.004547
  → Merged B: mean=-0.000004, std=0.000492

[MoELinear Layer] base_model.model.model.layers.27.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000015, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000008, std=0.001060
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000007, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000018, std=0.001266
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000032, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000028, std=0.000999
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000019, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000010, std=0.001144
  → Merged A: mean=-0.000009, std=0.004547
  → Merged B: mean=0.000007, std=0.000530

[MoELinear Layer] base_model.model.model.layers.27.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000065, std=0.009216
    B: shape=torch.Size([1024, 8]), mean=0.000006, std=0.001389
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000092, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000028, std=0.001289
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000004, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000014, std=0.001205
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000028, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000026, std=0.001427
  → Merged A: mean=0.000001, std=0.004578
  → Merged B: mean=-0.000002, std=0.000706

[MoELinear Layer] base_model.model.model.layers.27.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000049, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001373
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000026, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.001411
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000003, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001373
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000070, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.001366
  → Merged A: mean=-0.000001, std=0.004517
  → Merged B: mean=-0.000001, std=0.000591

[MoELinear Layer] base_model.model.model.layers.28.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000044, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000008, std=0.001228
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000000, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001221
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000155, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001144
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000033, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000005, std=0.001190
  → Merged A: mean=-0.000058, std=0.004547
  → Merged B: mean=-0.000003, std=0.000587

[MoELinear Layer] base_model.model.model.layers.28.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000011, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.000969
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000045, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000011, std=0.001152
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000051, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000003, std=0.001190
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000051, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000010, std=0.001129
  → Merged A: mean=0.000014, std=0.004578
  → Merged B: mean=0.000004, std=0.000584

[MoELinear Layer] base_model.model.model.layers.28.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000019, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000001, std=0.001129
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000048, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000029, std=0.001450
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000003, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000015, std=0.001122
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000006, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000003, std=0.001396
  → Merged A: mean=-0.000019, std=0.004547
  → Merged B: mean=0.000005, std=0.000614

[MoELinear Layer] base_model.model.model.layers.28.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000134, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001289
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000017, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001404
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000042, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001335
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000025, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.001328
  → Merged A: mean=-0.000042, std=0.004669
  → Merged B: mean=0.000002, std=0.000908

[MoELinear Layer] base_model.model.model.layers.29.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000016, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=-0.000001, std=0.000835
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000013, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001038
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000004, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001190
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000038, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001175
  → Merged A: mean=-0.000016, std=0.004517
  → Merged B: mean=0.000001, std=0.000553

[MoELinear Layer] base_model.model.model.layers.29.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000029, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000005, std=0.001236
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000006, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000018, std=0.001282
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000016, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.001030
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000080, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000010, std=0.001190
  → Merged A: mean=-0.000022, std=0.004547
  → Merged B: mean=-0.000001, std=0.000591

[MoELinear Layer] base_model.model.model.layers.29.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000008, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000013, std=0.001366
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000064, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000021, std=0.001495
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000017, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000006, std=0.001411
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000069, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000015, std=0.001251
  → Merged A: mean=-0.000001, std=0.004578
  → Merged B: mean=0.000003, std=0.000755

[MoELinear Layer] base_model.model.model.layers.29.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000000, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000006, std=0.001274
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000009, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000002, std=0.001328
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000032, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001381
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000046, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001358
  → Merged A: mean=-0.000017, std=0.004578
  → Merged B: mean=-0.000001, std=0.000744

[MoELinear Layer] base_model.model.model.layers.30.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000037, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.001259
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000001, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.001259
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000048, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001198
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000035, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001236
  → Merged A: mean=0.000011, std=0.004578
  → Merged B: mean=-0.000000, std=0.000622

[MoELinear Layer] base_model.model.model.layers.30.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000044, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000019, std=0.001198
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000045, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000009, std=0.001350
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000080, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000035, std=0.001144
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000046, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000007, std=0.001106
  → Merged A: mean=-0.000031, std=0.004517
  → Merged B: mean=-0.000005, std=0.000645

[MoELinear Layer] base_model.model.model.layers.30.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000012, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000039, std=0.001526
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000066, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000011, std=0.001297
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000008, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=-0.000040, std=0.001381
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000021, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000012, std=0.001320
  → Merged A: mean=-0.000006, std=0.004517
  → Merged B: mean=-0.000014, std=0.000580

[MoELinear Layer] base_model.model.model.layers.30.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000022, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000009, std=0.001389
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000036, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000000, std=0.001297
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000022, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000002, std=0.001373
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000050, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000004, std=0.001404
  → Merged A: mean=0.000022, std=0.004547
  → Merged B: mean=-0.000002, std=0.000572

[MoELinear Layer] base_model.model.model.layers.31.self_attn.q_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000046, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=0.000010, std=0.001129
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000031, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000000, std=0.001228
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000093, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000004, std=0.001144
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000050, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000003, std=0.001190
  → Merged A: mean=-0.000015, std=0.004547
  → Merged B: mean=0.000002, std=0.000553

[MoELinear Layer] base_model.model.model.layers.31.self_attn.k_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000071, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000014, std=0.001007
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000016, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.001030
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000064, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000004, std=0.001213
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000061, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=-0.000006, std=0.001129
  → Merged A: mean=0.000015, std=0.004486
  → Merged B: mean=0.000001, std=0.000492

[MoELinear Layer] base_model.model.model.layers.31.self_attn.v_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000007, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000006, std=0.001358
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000018, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000002, std=0.001358
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000048, std=0.009155
    B: shape=torch.Size([1024, 8]), mean=0.000003, std=0.001381
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000048, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=-0.000012, std=0.001244
  → Merged A: mean=-0.000003, std=0.004639
  → Merged B: mean=-0.000000, std=0.000820

[MoELinear Layer] base_model.model.model.layers.31.self_attn.o_proj
tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0')
  Expert 0 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000003, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000008, std=0.001305
  Expert 1 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=0.000012, std=0.009155
    B: shape=torch.Size([4096, 8]), mean=-0.000006, std=0.001266
  Expert 2 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000009, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=0.000001, std=0.001320
  Expert 3 (w=0.2500):
    A: shape=torch.Size([8, 4096]), mean=-0.000101, std=0.009094
    B: shape=torch.Size([4096, 8]), mean=-0.000000, std=0.001198
  → Merged A: mean=-0.000024, std=0.004578
  → Merged B: mean=-0.000003, std=0.000702
<class 'peft.tuners.lora.model.LoraModel'>
lora.Linear(
  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.05, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): Linear(in_features=4096, out_features=8, bias=False)
  )
  (lora_B): ModuleDict(
    (default): Linear(in_features=8, out_features=4096, bias=False)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
  (lora_magnitude_vector): ModuleDict()
)

========== MODEL STRUCTURE ==========
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaSdpaAttention(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (rotary_emb): LlamaRotaryEmbedding()
            )
            (mlp): LlamaMLP(
              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm()
            (post_attention_layernorm): LlamaRMSNorm()
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)

========== TRAINABLE PARAMETERS ==========
base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: shape=torch.Size([1024, 8]), numel=8192
base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768
base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: shape=torch.Size([8, 4096]), numel=32768
base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: shape=torch.Size([4096, 8]), numel=32768

Total trainable parameters: 6,815,744 / 8,037,076,992 (0.08%)

[LoraLinear Layer] base_model.model.model.layers.0.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000017, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.0.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000002, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.0.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000025, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.0.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000051, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.1.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000088, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.1.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000052, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.1.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000039, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.1.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000029, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.2.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000051, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.2.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000045, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.2.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000013, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.2.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000071, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.3.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000116, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.3.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000067, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.3.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000005, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.3.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000062, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.4.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000050, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.4.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000050, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.4.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000074, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.4.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000051, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.5.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000019, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.5.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000008, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.5.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000051, std=0.009094
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.5.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000031, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.6.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000052, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.6.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000029, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.6.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000056, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.6.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000011, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.7.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000023, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.7.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000013, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.7.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000022, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.7.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000006, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.8.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000046, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.8.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000043, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.8.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000036, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.8.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000028, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.9.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000082, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.9.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000082, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.9.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000029, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.9.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000034, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.10.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000035, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.10.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000002, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.10.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000010, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.10.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000038, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.11.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000104, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.11.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000036, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.11.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000027, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.11.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000081, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.12.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000048, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.12.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000091, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.12.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000020, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.12.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000007, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.13.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000023, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.13.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000045, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.13.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000012, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.13.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000106, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.14.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000049, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.14.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000061, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.14.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000065, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.14.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000030, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.15.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000099, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.15.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000052, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.15.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000083, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.15.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000122, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.16.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000072, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.16.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000062, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.16.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000025, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.16.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000009, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.17.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000017, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.17.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000012, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.17.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000032, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.17.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000058, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.18.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000047, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.18.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000016, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.18.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000007, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.18.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000059, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.19.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000051, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.19.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000075, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.19.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000026, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.19.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000090, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.20.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000067, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.20.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000084, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.20.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000146, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.20.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000118, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.21.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000000, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.21.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000000, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.21.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000029, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.21.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000006, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.22.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000059, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.22.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000104, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.22.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000000, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.22.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000008, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.23.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000064, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.23.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000027, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.23.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000012, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.23.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000028, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.24.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000055, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.24.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000038, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.24.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000009, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.24.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000014, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.25.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000061, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.25.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000062, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.25.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000003, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.25.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000013, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.26.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000026, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.26.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000040, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.26.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000013, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.26.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000037, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.27.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000061, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.27.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000019, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.27.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000035, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.27.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000013, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.28.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000003, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.28.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000081, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.28.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000001, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.28.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000018, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.29.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000067, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.29.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000010, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.29.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000117, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.29.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000053, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.30.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000002, std=0.008972
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.30.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000085, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.30.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000065, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.30.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000022, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.31.self_attn.q_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000006, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.31.self_attn.k_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000093, std=0.009033
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.31.self_attn.v_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=-0.000021, std=0.008972
    B: shape=torch.Size([1024, 8]), mean=0.000000, std=0.000000

[LoraLinear Layer] base_model.model.model.layers.31.self_attn.o_proj
  Adapter: default
    A: shape=torch.Size([8, 4096]), mean=0.000015, std=0.009033
    B: shape=torch.Size([4096, 8]), mean=0.000000, std=0.000000
/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
No files have been modified since last commit. Skipping to prevent empty commit.
05/06/2025 06:40:45 - WARNING - huggingface_hub.hf_api - No files have been modified since last commit. Skipping to prevent empty commit.
