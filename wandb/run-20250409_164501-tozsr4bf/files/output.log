

  0%|                                                                                                      | 1/3150 [00:03<3:11:35,  3.65s/it]
{'loss': 1.3356, 'grad_norm': 0.205078125, 'learning_rate': 3.9987301587301595e-05, 'epoch': 0.0}


  0%|                                                                                                      | 3/3150 [00:09<2:48:40,  3.22s/it]
{'loss': 1.4629, 'grad_norm': 0.216796875, 'learning_rate': 3.9961904761904766e-05, 'epoch': 0.0}

  0%|▏                                                                                                     | 4/3150 [00:12<2:46:06,  3.17s/it]


  0%|▏                                                                                                     | 6/3150 [00:19<2:44:12,  3.13s/it]
{'loss': 1.3195, 'grad_norm': 0.2490234375, 'learning_rate': 3.992380952380953e-05, 'epoch': 0.0}


  0%|▎                                                                                                     | 8/3150 [00:25<2:43:31,  3.12s/it]
{'loss': 1.2607, 'grad_norm': 0.279296875, 'learning_rate': 3.9898412698412706e-05, 'epoch': 0.0}


  0%|▎                                                                                                    | 10/3150 [00:31<2:42:40,  3.11s/it]
{'loss': 1.3864, 'grad_norm': 0.380859375, 'learning_rate': 3.9873015873015876e-05, 'epoch': 0.0}

  0%|▎                                                                                                    | 11/3150 [00:34<2:42:32,  3.11s/it]

  0%|▍                                                                                                    | 12/3150 [00:37<2:42:27,  3.11s/it]

  0%|▍                                                                                                    | 13/3150 [00:40<2:42:28,  3.11s/it]

  0%|▍                                                                                                    | 14/3150 [00:43<2:42:32,  3.11s/it]

  0%|▍                                                                                                    | 15/3150 [00:47<2:42:36,  3.11s/it]


  1%|▌                                                                                                    | 17/3150 [00:53<2:42:38,  3.11s/it]
{'loss': 1.2654, 'grad_norm': 0.470703125, 'learning_rate': 3.978412698412699e-05, 'epoch': 0.01}


  1%|▌                                                                                                    | 19/3150 [00:59<2:42:48,  3.12s/it]
{'loss': 1.1905, 'grad_norm': 0.435546875, 'learning_rate': 3.9758730158730164e-05, 'epoch': 0.01}

  1%|▋                                                                                                    | 20/3150 [01:02<2:42:53,  3.12s/it]

  1%|▋                                                                                                    | 21/3150 [01:05<2:42:58,  3.13s/it]

  1%|▋                                                                                                    | 22/3150 [01:08<2:42:56,  3.13s/it]


  1%|▊                                                                                                    | 24/3150 [01:15<2:43:06,  3.13s/it]
{'loss': 1.174, 'grad_norm': 0.447265625, 'learning_rate': 3.96952380952381e-05, 'epoch': 0.01}


  1%|▊                                                                                                    | 26/3150 [01:21<2:43:13,  3.13s/it]
{'loss': 1.1727, 'grad_norm': 0.486328125, 'learning_rate': 3.9669841269841275e-05, 'epoch': 0.01}


  1%|▉                                                                                                    | 28/3150 [01:27<2:43:17,  3.14s/it]
{'loss': 1.1205, 'grad_norm': 0.40234375, 'learning_rate': 3.964444444444445e-05, 'epoch': 0.01}

  1%|▉                                                                                                    | 29/3150 [01:30<2:43:17,  3.14s/it]

  1%|▉                                                                                                    | 30/3150 [01:34<2:43:19,  3.14s/it]

  1%|▉                                                                                                    | 31/3150 [01:37<2:43:20,  3.14s/it]


  1%|█                                                                                                    | 33/3150 [01:43<2:43:20,  3.14s/it]
{'loss': 1.0194, 'grad_norm': 0.341796875, 'learning_rate': 3.9580952380952385e-05, 'epoch': 0.01}


  1%|█                                                                                                    | 35/3150 [01:49<2:43:26,  3.15s/it]
{'loss': 0.9821, 'grad_norm': 0.3359375, 'learning_rate': 3.955555555555556e-05, 'epoch': 0.01}

  1%|█▏                                                                                                   | 36/3150 [01:52<2:43:25,  3.15s/it]

  1%|█▏                                                                                                   | 37/3150 [01:56<2:43:21,  3.15s/it]

  1%|█▏                                                                                                   | 38/3150 [01:59<2:43:21,  3.15s/it]


  1%|█▎                                                                                                   | 40/3150 [02:05<2:43:24,  3.15s/it]
{'loss': 0.9515, 'grad_norm': 0.302734375, 'learning_rate': 3.9492063492063495e-05, 'epoch': 0.01}


  1%|█▎                                                                                                   | 42/3150 [02:11<2:43:21,  3.15s/it]
{'loss': 0.8474, 'grad_norm': 0.259765625, 'learning_rate': 3.946666666666667e-05, 'epoch': 0.01}

  1%|█▍                                                                                                   | 43/3150 [02:15<2:43:20,  3.15s/it]

  1%|█▍                                                                                                   | 44/3150 [02:18<2:43:17,  3.15s/it]

  1%|█▍                                                                                                   | 45/3150 [02:21<2:43:18,  3.16s/it]


  1%|█▌                                                                                                   | 47/3150 [02:27<2:43:16,  3.16s/it]
{'loss': 0.8975, 'grad_norm': 0.306640625, 'learning_rate': 3.9403174603174606e-05, 'epoch': 0.01}

  2%|█▌                                                                                                   | 48/3150 [02:30<2:43:17,  3.16s/it]

  2%|█▌                                                                                                   | 49/3150 [02:34<2:43:19,  3.16s/it]

  2%|█▌                                                                                                   | 50/3150 [02:37<2:43:19,  3.16s/it]


  2%|█▋                                                                                                   | 52/3150 [02:43<2:43:14,  3.16s/it]
{'loss': 0.9037, 'grad_norm': 0.234375, 'learning_rate': 3.9339682539682546e-05, 'epoch': 0.02}


  2%|█▋                                                                                                   | 54/3150 [02:49<2:43:10,  3.16s/it]
{'loss': 0.7659, 'grad_norm': 0.228515625, 'learning_rate': 3.9314285714285716e-05, 'epoch': 0.02}

  2%|█▊                                                                                                   | 55/3150 [02:53<2:43:11,  3.16s/it]

  2%|█▊                                                                                                   | 56/3150 [02:56<2:43:09,  3.16s/it]

  2%|█▊                                                                                                   | 57/3150 [02:59<2:43:10,  3.17s/it]


  2%|█▉                                                                                                   | 59/3150 [03:05<2:43:06,  3.17s/it]
{'loss': 0.7874, 'grad_norm': 0.2080078125, 'learning_rate': 3.9250793650793656e-05, 'epoch': 0.02}


  2%|█▉                                                                                                   | 61/3150 [03:12<2:43:01,  3.17s/it]
{'loss': 0.8639, 'grad_norm': 0.2294921875, 'learning_rate': 3.922539682539683e-05, 'epoch': 0.02}

  2%|█▉                                                                                                   | 62/3150 [03:15<2:43:07,  3.17s/it]

  2%|██                                                                                                   | 63/3150 [03:18<2:43:00,  3.17s/it]

  2%|██                                                                                                   | 64/3150 [03:21<2:42:54,  3.17s/it]


  2%|██                                                                                                   | 66/3150 [03:27<2:42:42,  3.17s/it]
{'loss': 0.8526, 'grad_norm': 0.19921875, 'learning_rate': 3.916190476190477e-05, 'epoch': 0.02}

  2%|██▏                                                                                                  | 67/3150 [03:31<2:42:42,  3.17s/it]

  2%|██▏                                                                                                  | 68/3150 [03:34<2:42:41,  3.17s/it]

  2%|██▏                                                                                                  | 69/3150 [03:37<2:42:37,  3.17s/it]


  2%|██▎                                                                                                  | 71/3150 [03:43<2:42:31,  3.17s/it]
{'loss': 0.8262, 'grad_norm': 0.1953125, 'learning_rate': 3.90984126984127e-05, 'epoch': 0.02}


  2%|██▎                                                                                                  | 73/3150 [03:50<2:42:30,  3.17s/it]
{'loss': 0.8425, 'grad_norm': 0.19140625, 'learning_rate': 3.907301587301588e-05, 'epoch': 0.02}

  2%|██▎                                                                                                  | 74/3150 [03:53<2:42:25,  3.17s/it]

  2%|██▍                                                                                                  | 75/3150 [03:56<2:42:22,  3.17s/it]

  2%|██▍                                                                                                  | 76/3150 [03:59<2:42:18,  3.17s/it]


  2%|██▌                                                                                                  | 78/3150 [04:05<2:42:07,  3.17s/it]
{'loss': 0.8527, 'grad_norm': 0.20703125, 'learning_rate': 3.900952380952381e-05, 'epoch': 0.02}


  3%|██▌                                                                                                  | 80/3150 [04:12<2:42:04,  3.17s/it]
{'loss': 0.8318, 'grad_norm': 0.203125, 'learning_rate': 3.898412698412699e-05, 'epoch': 0.03}

  3%|██▌                                                                                                  | 81/3150 [04:15<2:42:00,  3.17s/it]

  3%|██▋                                                                                                  | 82/3150 [04:18<2:41:57,  3.17s/it]

  3%|██▋                                                                                                  | 83/3150 [04:21<2:42:09,  3.17s/it]


  3%|██▋                                                                                                  | 85/3150 [04:28<2:41:55,  3.17s/it]
{'loss': 0.7849, 'grad_norm': 0.2314453125, 'learning_rate': 3.892063492063492e-05, 'epoch': 0.03}

  3%|██▊                                                                                                  | 86/3150 [04:31<2:41:54,  3.17s/it]

  3%|██▊                                                                                                  | 87/3150 [04:34<2:41:57,  3.17s/it]

  3%|██▊                                                                                                  | 88/3150 [04:37<2:41:55,  3.17s/it]


  3%|██▉                                                                                                  | 90/3150 [04:43<2:41:43,  3.17s/it]
{'loss': 0.786, 'grad_norm': 0.2021484375, 'learning_rate': 3.885714285714286e-05, 'epoch': 0.03}


  3%|██▉                                                                                                  | 92/3150 [04:50<2:41:31,  3.17s/it]
{'loss': 0.7822, 'grad_norm': 0.2177734375, 'learning_rate': 3.883174603174603e-05, 'epoch': 0.03}

  3%|██▉                                                                                                  | 93/3150 [04:53<2:41:26,  3.17s/it]

  3%|███                                                                                                  | 94/3150 [04:56<2:41:23,  3.17s/it]

  3%|███                                                                                                  | 95/3150 [04:59<2:41:21,  3.17s/it]


  3%|███                                                                                                  | 97/3150 [05:06<2:41:15,  3.17s/it]
{'loss': 0.7832, 'grad_norm': 0.2255859375, 'learning_rate': 3.876825396825397e-05, 'epoch': 0.03}

  3%|███▏                                                                                                 | 98/3150 [05:09<2:41:09,  3.17s/it]

  3%|███▏                                                                                                 | 99/3150 [05:12<2:41:04,  3.17s/it]

  3%|███▏                                                                                                | 100/3150 [05:15<2:41:02,  3.17s/it]


  3%|███▏                                                                                                | 102/3150 [05:21<2:40:55,  3.17s/it]
{'loss': 0.7622, 'grad_norm': 0.20703125, 'learning_rate': 3.8704761904761904e-05, 'epoch': 0.03}


  3%|███▎                                                                                                | 104/3150 [05:28<2:40:51,  3.17s/it]
{'loss': 0.7606, 'grad_norm': 0.1884765625, 'learning_rate': 3.867936507936508e-05, 'epoch': 0.03}

  3%|███▎                                                                                                | 105/3150 [05:31<2:40:44,  3.17s/it]

  3%|███▎                                                                                                | 106/3150 [05:34<2:40:41,  3.17s/it]

  3%|███▍                                                                                                | 107/3150 [05:37<2:40:37,  3.17s/it]


  3%|███▍                                                                                                | 109/3150 [05:44<2:40:29,  3.17s/it]
{'loss': 0.7767, 'grad_norm': 0.2265625, 'learning_rate': 3.8615873015873015e-05, 'epoch': 0.03}


  4%|███▌                                                                                                | 111/3150 [05:50<2:40:23,  3.17s/it]
{'loss': 0.7793, 'grad_norm': 0.234375, 'learning_rate': 3.859047619047619e-05, 'epoch': 0.04}

  4%|███▌                                                                                                | 112/3150 [05:53<2:40:20,  3.17s/it]

  4%|███▌                                                                                                | 113/3150 [05:56<2:40:25,  3.17s/it]

  4%|███▌                                                                                                | 114/3150 [05:59<2:40:34,  3.17s/it]


  4%|███▋                                                                                                | 116/3150 [06:06<2:40:29,  3.17s/it]
{'loss': 0.7752, 'grad_norm': 0.220703125, 'learning_rate': 3.8526984126984125e-05, 'epoch': 0.04}

  4%|███▋                                                                                                | 117/3150 [06:09<2:40:23,  3.17s/it]

  4%|███▋                                                                                                | 118/3150 [06:12<2:40:16,  3.17s/it]

  4%|███▊                                                                                                | 119/3150 [06:15<2:40:12,  3.17s/it]


  4%|███▊                                                                                                | 121/3150 [06:22<2:40:05,  3.17s/it]
{'loss': 0.777, 'grad_norm': 0.2412109375, 'learning_rate': 3.8463492063492065e-05, 'epoch': 0.04}


  4%|███▉                                                                                                | 123/3150 [06:28<2:39:59,  3.17s/it]
  File "/data/home/yjw5427/translora/fed_lora.py", line 962, in <module>                                 | 123/3150 [06:28<2:39:59,  3.17s/it]
    main()
  File "/data/home/yjw5427/translora/fed_lora.py", line 800, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 3485, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 3532, in compute_loss
    outputs = model(**inputs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/peft_model.py", line 1430, in forward
    return self.base_model(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 179, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1189, in forward
    outputs = self.model(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1000, in forward
    layer_outputs = decoder_layer(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 744, in forward
    hidden_states = self.post_attention_layernorm(hidden_states)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in __getattr__
    def __getattr__(self, name: str) -> Any:
KeyboardInterrupt
Traceback (most recent call last):
  File "/data/home/yjw5427/translora/fed_lora.py", line 962, in <module>
    main()
  File "/data/home/yjw5427/translora/fed_lora.py", line 800, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 3485, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 3532, in compute_loss
    outputs = model(**inputs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/peft_model.py", line 1430, in forward
    return self.base_model(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 179, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1189, in forward
    outputs = self.model(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1000, in forward
    layer_outputs = decoder_layer(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 744, in forward
    hidden_states = self.post_attention_layernorm(hidden_states)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in __getattr__
    def __getattr__(self, name: str) -> Any:
KeyboardInterrupt