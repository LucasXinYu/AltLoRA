


























/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-67f45957-6770564a3c8af694678d7aee;a8cd837e-6388-4447-92ae-d93a4c12843b)
Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-3B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B to ask for access. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B.
  warnings.warn(
/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B - will assume that the vocabulary was not modified.
  warnings.warn(
{'train_runtime': 63.0666, 'train_samples_per_second': 3.488, 'train_steps_per_second': 0.428, 'train_loss': 1.616707271999783, 'epoch': 0.98}
***** train metrics *****
  epoch                    =     0.9818
  total_flos               =  3543966GF
  train_loss               =     1.6167
  train_runtime            = 0:01:03.06
  train_samples            =        220
  train_samples_per_second =      3.488
  train_steps_per_second   =      0.428
15
1.2e-06


























 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████    | 26/27 [00:53<00:02,  2.08s/it]
{'train_runtime': 56.0026, 'train_samples_per_second': 3.893, 'train_steps_per_second': 0.482, 'train_loss': 1.6098240039966725, 'epoch': 0.99}
***** train metrics *****
  epoch                    =     0.9908
  total_flos               =  3543966GF
  train_loss               =     1.6098
  train_runtime            = 0:00:56.00
  train_samples            =        218
  train_samples_per_second =      3.893
  train_steps_per_second   =      0.482
16
/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-67f45990-72724c466d0921dc15c10a3a;2492de5a-6cc8-40d6-a29f-b8b11f8ec14f)
Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-3B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B to ask for access. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B.
  warnings.warn(
/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B - will assume that the vocabulary was not modified.
  warnings.warn(



























 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████▏   | 27/28 [00:56<00:02,  2.10s/it]
{'train_runtime': 58.6385, 'train_samples_per_second': 3.837, 'train_steps_per_second': 0.478, 'train_loss': 1.6065160206386022, 'epoch': 0.99}
***** train metrics *****
  epoch                    =     0.9912
  total_flos               =  3675224GF
  train_loss               =     1.6065
  train_runtime            = 0:00:58.63
  train_samples            =        225
  train_samples_per_second =      3.837
  train_steps_per_second   =      0.478
15
/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-67f459cc-113b16172a67376d2aed2a9c;2baaf85f-5809-47cf-baec-8f61e96514ec)
Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-3B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B to ask for access. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B.
  warnings.warn(
/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B - will assume that the vocabulary was not modified.
  warnings.warn(


























 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████    | 26/27 [00:54<00:02,  2.10s/it]
{'train_runtime': 56.7831, 'train_samples_per_second': 3.91, 'train_steps_per_second': 0.475, 'train_loss': 1.6113001505533855, 'epoch': 0.97}
***** train metrics *****
  epoch                    =      0.973
  total_flos               =  3543966GF
  train_loss               =     1.6113
  train_runtime            = 0:00:56.78
  train_samples            =        222
  train_samples_per_second =       3.91
  train_steps_per_second   =      0.475
15
/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-67f45a06-2bb6ed5e780b933a1a7571a8;5c980818-6830-49e0-8d0f-2b10fd03f096)
Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-3B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B to ask for access. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B.
  warnings.warn(
/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B - will assume that the vocabulary was not modified.
  warnings.warn(


























 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████    | 26/27 [00:54<00:02,  2.10s/it]
{'train_runtime': 56.8, 'train_samples_per_second': 3.873, 'train_steps_per_second': 0.475, 'train_loss': 1.6271787572790075, 'epoch': 0.98}
***** train metrics *****
  epoch                    =     0.9818
  total_flos               =  3543966GF
  train_loss               =     1.6272
  train_runtime            = 0:00:56.79
  train_samples            =        220
  train_samples_per_second =      3.873
/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error 403 Client Error. (Request ID: Root=1-67f45a40-1c0e9fdc65d4c662580a387b;e81ec338-5bef-46f7-b1a2-669f1ce8a368)
Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-3B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B to ask for access. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-3B.
  warnings.warn(
/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in meta-llama/Llama-3.2-3B - will assume that the vocabulary was not modified.
  warnings.warn(












[WARNING|logging.py:328] 2025-04-07 19:06:05,689 >> The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Traceback (most recent call last):
  File "/data/home/yjw5427/translora/fed_lora.py", line 933, in <module>
    main()
  File "/data/home/yjw5427/translora/fed_lora.py", line 861, in main
    res = generate_text(prompt)
  File "/data/home/yjw5427/translora/fed_lora.py", line 844, in generate_text
    outputs = model.generate(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/peft_model.py", line 1491, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/generation/utils.py", line 1905, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/generation/utils.py", line 1228, in _validate_generated_length
    raise ValueError(
ValueError: Input length of input_ids is 211, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
Traceback (most recent call last):
  File "/data/home/yjw5427/translora/fed_lora.py", line 933, in <module>
    main()
  File "/data/home/yjw5427/translora/fed_lora.py", line 861, in main
    res = generate_text(prompt)
  File "/data/home/yjw5427/translora/fed_lora.py", line 844, in generate_text
    outputs = model.generate(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/peft/peft_model.py", line 1491, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/generation/utils.py", line 1905, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/generation/utils.py", line 1228, in _validate_generated_length
    raise ValueError(
ValueError: Input length of input_ids is 211, but `max_length` is set to 128. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.
***** eval metrics *****
  epoch                   =     0.9818
  eval_accuracy           =     0.7417
  eval_invalid_labels     =     244731
  eval_invalid_preds      =     242222
  eval_loss               =     1.5401
  eval_runtime            = 0:00:28.81
  eval_samples            =        251
  eval_samples_per_second =      8.711
  eval_steps_per_second   =      1.111
  eval_total              =      12042
  perplexity              =     4.6652