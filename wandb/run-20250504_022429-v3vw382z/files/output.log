05/04/2025 02:24:30 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[WARNING|logging.py:314] 2025-05-04 02:24:40,861 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
flash_attention_2
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.08it/s]
Model dtype: torch.bfloat16
Number of layers (parameter sets) in the model: 291
max_gate_samples is 50
trainable params: 29,360,128 || all params: 8,059,621,376 || trainable%: 0.3643
['query', 'response', 'type', 'original_question']
  0%|                                                                                                                                                                                  | 0/1 [00:00<?, ?it/s][WARNING|logging.py:329] 2025-05-04 02:25:07,801 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
==========================================
True
==========================================
=================================================================================
Adamw
8
False
=================================================================================
=================================================================================
SchedulerType.COSINE
=================================================================================
checkpoint
None
/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:15<00:00,  7.58s/it]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:19<00:00, 79.06s/it]
{'loss': 1.272, 'learning_rate': 2.9341204441673266e-05, 'epoch': 0.0}
{'loss': 1.2628, 'learning_rate': 0.0, 'epoch': 0.01}
{'train_runtime': 75.8479, 'train_samples_per_second': 4.219, 'train_steps_per_second': 0.132, 'train_loss': 1.2673697471618652, 'epoch': 0.01}
***** train metrics *****
  epoch                    =       0.01
  train_loss               =     1.2674
  train_runtime            = 0:01:15.84
  train_samples            =      44056
  train_samples_per_second =      4.219
  train_steps_per_second   =      0.132
/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/root/autodl-tmp/aslora_new/main_lora_math_3_8b_moelora.py", line 840, in <module>
    main()
  File "/root/autodl-tmp/aslora_new/main_lora_math_3_8b_moelora.py", line 822, in main
    model.push_to_hub(hf_repo_id, use_auth_token=hf_write_token)
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py", line 876, in push_to_hub
    self.save_pretrained(work_dir, max_shard_size=max_shard_size, safe_serialization=safe_serialization)
  File "/root/autodl-tmp/aslora_new/peft/src/peft/peft_model.py", line 294, in save_pretrained
    output_state_dict = get_peft_model_state_dict(
  File "/root/autodl-tmp/aslora_new/peft/src/peft/utils/save_and_load.py", line 186, in get_peft_model_state_dict
    raise ValueError(f"Unknown PEFT type passed: {config.peft_type}")
ValueError: Unknown PEFT type passed: MOELORA
