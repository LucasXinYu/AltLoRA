

  0%|▎                                                                 | 1/208 [00:02<09:00,  2.61s/it]

  1%|▋                                                                 | 2/208 [00:04<07:48,  2.28s/it]

  1%|▉                                                                 | 3/208 [00:06<07:24,  2.17s/it]

  2%|█▎                                                                | 4/208 [00:08<07:12,  2.12s/it]

  2%|█▌                                                                | 5/208 [00:10<07:05,  2.10s/it]

  3%|█▉                                                                | 6/208 [00:12<07:01,  2.09s/it]

  3%|██▏                                                               | 7/208 [00:14<06:58,  2.08s/it]

  4%|██▌                                                               | 8/208 [00:17<06:56,  2.08s/it]

  4%|██▊                                                               | 9/208 [00:19<06:53,  2.08s/it]

  5%|███▏                                                             | 10/208 [00:21<06:51,  2.08s/it]

  5%|███▍                                                             | 11/208 [00:23<06:49,  2.08s/it]

  6%|███▊                                                             | 12/208 [00:25<06:47,  2.08s/it]

  6%|████                                                             | 13/208 [00:27<06:45,  2.08s/it]

  7%|████▍                                                            | 14/208 [00:29<06:42,  2.08s/it]

  7%|████▋                                                            | 15/208 [00:31<06:40,  2.08s/it]

  8%|█████                                                            | 16/208 [00:33<06:38,  2.08s/it]

  8%|█████▎                                                           | 17/208 [00:35<06:36,  2.07s/it]
{'loss': 2.2541, 'grad_norm': 0.494140625, 'learning_rate': 1.1634517766497462e-06, 'epoch': 0.08}

  9%|█████▋                                                           | 18/208 [00:37<06:34,  2.07s/it]

  9%|█████▉                                                           | 19/208 [00:39<06:32,  2.07s/it]

 10%|██████▎                                                          | 20/208 [00:41<06:30,  2.08s/it]

 10%|██████▌                                                          | 21/208 [00:44<06:28,  2.08s/it]
{'loss': 2.1515, 'grad_norm': 0.482421875, 'learning_rate': 1.132994923857868e-06, 'epoch': 0.11}
 11%|██████▉                                                          | 22/208 [00:46<06:26,  2.08s/it]


 12%|███████▌                                                         | 24/208 [00:50<06:21,  2.07s/it]

 12%|███████▊                                                         | 25/208 [00:52<06:19,  2.07s/it]

 12%|████████▏                                                        | 26/208 [00:54<06:17,  2.07s/it]
{'loss': 2.1001, 'grad_norm': 0.54296875, 'learning_rate': 1.1086294416243654e-06, 'epoch': 0.12}

 13%|████████▍                                                        | 27/208 [00:56<06:15,  2.07s/it]

 13%|████████▊                                                        | 28/208 [00:58<06:13,  2.07s/it]

 14%|█████████                                                        | 29/208 [01:00<06:11,  2.07s/it]


 15%|█████████▋                                                       | 31/208 [01:04<06:07,  2.07s/it]

 15%|██████████                                                       | 32/208 [01:06<06:05,  2.08s/it]

 16%|██████████▎                                                      | 33/208 [01:08<06:03,  2.08s/it]

 16%|██████████▋                                                      | 34/208 [01:10<06:01,  2.08s/it]

 17%|██████████▉                                                      | 35/208 [01:13<05:59,  2.08s/it]

 17%|███████████▎                                                     | 36/208 [01:15<05:57,  2.08s/it]
  File "/data/home/yjw5427/translora/fed_lora.py", line 958, in <module>37/208 [01:17<05:55,  2.08s/it]
    main()
  File "/data/home/yjw5427/translora/fed_lora.py", line 798, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 3518, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/accelerator.py", line 2241, in backward
    loss.backward(**kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "/data/home/yjw5427/translora/fed_lora.py", line 958, in <module>
    main()
  File "/data/home/yjw5427/translora/fed_lora.py", line 798, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/transformers/trainer.py", line 3518, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/accelerate/accelerator.py", line 2241, in backward
    loss.backward(**kwargs)
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/yjw5427/miniconda3/envs/trans/lib/python3.10/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
{'loss': 2.2892, 'grad_norm': 0.498046875, 'learning_rate': 1.0416243654822335e-06, 'epoch': 0.18}