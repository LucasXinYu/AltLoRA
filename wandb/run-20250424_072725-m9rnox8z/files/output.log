04/24/2025 07:27:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[WARNING|logging.py:314] 2025-04-24 07:27:35,891 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
flash_attention_2
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.53it/s]
Model dtype: torch.bfloat16
Number of layers (parameter sets) in the model: 291
max_gate_samples is 50
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
['id', 'messages']
Running tokenizer on dataset: 100%|████████████████████████████████████████████████████████████████████████████████████████| 6539/6539 [00:06<00:00, 936.65 examples/s]
Grouping texts in chunks of 512: 100%|█████████████████████████████████████████████████████████████████████████████████████| 6539/6539 [00:11<00:00, 562.38 examples/s]
  0%|                                                                                                                                            | 0/1 [00:00<?, ?it/s][WARNING|logging.py:329] 2025-04-24 07:28:09,465 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
==========================================
True
==========================================
=================================================================================
adamwr
8
False
=================================================================================
=================================================================================
SchedulerType.COSINE
=================================================================================
create_optimizer
checkpoint
None
/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/aslora_new/optimizer_new.py:777: UserWarning: This overload of addcdiv_ is deprecated:
	addcdiv_(Number value, Tensor tensor1, Tensor tensor2)
Consider using one of the following signatures instead:
	addcdiv_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1519.)
  p1.data.addcdiv_(-step_size, beta1 * P_at @ exp_avg + (1-beta1) * grad1_scaled,denom)
                                                                                                                                                                       
                                                                                                                                                                       
{'loss': 3.2015, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.0}
{'loss': 1.958, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.0}
{'loss': 1.7105, 'learning_rate': 5e-06, 'epoch': 0.0}
{'loss': 1.5925, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.0}
{'loss': 1.4702, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}
{'loss': 1.4345, 'learning_rate': 1e-05, 'epoch': 0.01}
{'loss': 1.5174, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.01}
{'loss': 1.4321, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.01}
{'loss': 1.345, 'learning_rate': 1.5e-05, 'epoch': 0.01}
{'loss': 1.4265, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.01}
{'loss': 1.3561, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.01}
{'loss': 1.3527, 'learning_rate': 2e-05, 'epoch': 0.01}
{'loss': 1.3187, 'learning_rate': 2.1666666666666667e-05, 'epoch': 0.01}
{'loss': 1.2608, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.02}
{'loss': 1.3061, 'learning_rate': 2.5e-05, 'epoch': 0.02}
{'loss': 1.349, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.02}
{'loss': 1.2905, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.02}
{'loss': 1.214, 'learning_rate': 3e-05, 'epoch': 0.02}
{'loss': 1.2373, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.02}
{'loss': 1.3162, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.02}
{'loss': 1.1908, 'learning_rate': 3.5e-05, 'epoch': 0.02}
{'loss': 1.2526, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.02}
{'loss': 1.2336, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.03}
{'loss': 1.2141, 'learning_rate': 4e-05, 'epoch': 0.03}
{'loss': 1.227, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.03}
{'loss': 1.2275, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.03}
{'loss': 1.2457, 'learning_rate': 4.5e-05, 'epoch': 0.03}
{'loss': 1.1794, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.03}
{'loss': 1.1563, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.03}
{'loss': 1.2732, 'learning_rate': 5e-05, 'epoch': 0.03}
{'loss': 1.2549, 'learning_rate': 4.999962028395596e-05, 'epoch': 0.03}
{'loss': 1.2066, 'learning_rate': 4.999848114735858e-05, 'epoch': 0.04}
{'loss': 1.207, 'learning_rate': 4.9996582624811725e-05, 'epoch': 0.04}
{'loss': 1.2267, 'learning_rate': 4.999392477398737e-05, 'epoch': 0.04}
{'loss': 1.2243, 'learning_rate': 4.999050767562379e-05, 'epoch': 0.04}
{'loss': 1.2167, 'learning_rate': 4.9986331433523156e-05, 'epoch': 0.04}
{'loss': 1.2632, 'learning_rate': 4.9981396174548355e-05, 'epoch': 0.04}
{'loss': 1.1893, 'learning_rate': 4.997570204861915e-05, 'epoch': 0.04}
{'loss': 1.2364, 'learning_rate': 4.996924922870762e-05, 'epoch': 0.04}
{'loss': 1.2694, 'learning_rate': 4.996203791083291e-05, 'epoch': 0.04}
{'loss': 1.1831, 'learning_rate': 4.9954068314055255e-05, 'epoch': 0.05}
{'loss': 1.1612, 'learning_rate': 4.994534068046937e-05, 'epoch': 0.05}
{'loss': 1.2297, 'learning_rate': 4.993585527519704e-05, 'epoch': 0.05}
{'loss': 1.2777, 'learning_rate': 4.992561238637912e-05, 'epoch': 0.05}
{'loss': 1.2579, 'learning_rate': 4.991461232516675e-05, 'epoch': 0.05}
{'loss': 1.1593, 'learning_rate': 4.9902855425711905e-05, 'epoch': 0.05}
{'loss': 1.2114, 'learning_rate': 4.9890342045157245e-05, 'epoch': 0.05}
{'loss': 1.1728, 'learning_rate': 4.9877072563625285e-05, 'epoch': 0.05}
{'loss': 1.2281, 'learning_rate': 4.9863047384206835e-05, 'epoch': 0.05}
{'loss': 1.1787, 'learning_rate': 4.984826693294874e-05, 'epoch': 0.06}
{'loss': 1.1588, 'learning_rate': 4.9832731658840956e-05, 'epoch': 0.06}
{'loss': 1.2147, 'learning_rate': 4.981644203380291e-05, 'epoch': 0.06}
{'loss': 1.24, 'learning_rate': 4.9799398552669175e-05, 'epoch': 0.06}
{'loss': 1.1757, 'learning_rate': 4.978160173317438e-05, 'epoch': 0.06}
{'loss': 1.2186, 'learning_rate': 4.976305211593758e-05, 'epoch': 0.06}
{'loss': 1.1677, 'learning_rate': 4.974375026444575e-05, 'epoch': 0.06}
{'loss': 1.1578, 'learning_rate': 4.972369676503672e-05, 'epoch': 0.06}
{'loss': 1.1915, 'learning_rate': 4.970289222688129e-05, 'epoch': 0.07}
{'loss': 1.1955, 'learning_rate': 4.968133728196486e-05, 'epoch': 0.07}
{'loss': 1.1365, 'learning_rate': 4.965903258506806e-05, 'epoch': 0.07}
{'loss': 1.2339, 'learning_rate': 4.963597881374702e-05, 'epoch': 0.07}
