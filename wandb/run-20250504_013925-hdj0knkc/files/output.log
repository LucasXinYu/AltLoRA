05/04/2025 01:39:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
README.md: 4.45kB [00:00, 6.08MB/s]                                                                                                                                                                          
MetaMathQA-395K.json: 396MB [00:12, 32.9MB/s] 
Generating train split: 395000 examples [00:06, 56976.24 examples/s]
/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[WARNING|logging.py:314] 2025-05-04 01:40:03,562 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
flash_attention_2
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.19it/s]
Model dtype: torch.bfloat16
Number of layers (parameter sets) in the model: 291
max_gate_samples is 50
Traceback (most recent call last):
  File "/root/autodl-tmp/aslora_new/main_lora_math_3_8b_moelora.py", line 839, in <module>
    main()
  File "/root/autodl-tmp/aslora_new/main_lora_math_3_8b_moelora.py", line 550, in main
    model = get_peft_model(model, lora_config)
  File "/root/autodl-tmp/aslora_new/peft/src/peft/mapping.py", line 184, in get_peft_model
    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](
  File "/root/autodl-tmp/aslora_new/peft/src/peft/peft_model.py", line 1520, in __init__
    super().__init__(model, peft_config, adapter_name, **kwargs)
  File "/root/autodl-tmp/aslora_new/peft/src/peft/peft_model.py", line 156, in __init__
    cls = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type]
KeyError: None
