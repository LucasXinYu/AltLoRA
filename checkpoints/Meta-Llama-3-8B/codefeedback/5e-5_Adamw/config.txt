--model_name_or_path meta-llama/Meta-Llama-3-8B --output_dir /root/autodl-tmp/aslora_new/checkpoints/Meta-Llama-3-8B/codefeedback/5e-5_Adamw --dataset_name meta-math/MetaMathQA --dataset_config_name default --per_device_train_batch_size 4 --per_device_eval_batch_size 1 --max_steps 1000 --overwrite_output_dir --do_train True --do_eval --lr_scheduler_type cosine --block_size 512 --seed 42 --fp16 True --gradient_checkpointing True --local_rank 8 --dataloader_num_workers 16 --disable_tqdm False --save_strategy no --evaluation_strategy epoch --load_best_model_at_end True --learning_rate 5e-5 --optim_notes Adamw --split_strategy iid --num_rounds 1 --num_clients 1 --sample_clients 1 --max_gate_samples 50 --max_train_samples 100000 --gradient_accumulation_steps 8
